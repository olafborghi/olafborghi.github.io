[
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "resources/index.html#the-languages-of-middle-earth",
    "href": "resources/index.html#the-languages-of-middle-earth",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "resources/index.html#the-history-of-the-war-of-the-ring",
    "href": "resources/index.html#the-history-of-the-war-of-the-ring",
    "title": "Projects",
    "section": "The History of the War of the Ring",
    "text": "The History of the War of the Ring\n\narXiv Preprint | Code\nI am creating a comprehensive and detailed history of the conflict that goes beyond the surface-level events. By gathering information from a variety of sources, including my own memories, written accounts, and oral histories, I hope to shed new light on this important period in Middle-earth’s history and provide valuable insights into the motivations and actions of the various players involved.\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Here you can find my published journal articles, some working papers and preprints, and other information about past and current projects. For a full list you may also refer to my CV."
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Research",
    "section": "Journal articles",
    "text": "Journal articles\n\n\n\n    \n        \n            \n                Borghi, O., Voracek, M., & Tran, U. S. (2024). Day-to-day associations between mindfulness and perceived stress: Insights from random intercept cross-lagged panel modeling. Frontiers in Psychology, 15. https://doi.org/10.3389/fpsyg.2024.1272720\n            \n\n            \n            \n                \n                    \n                            Mindfulness\n                        \n                    \n                    \n                            Perceived stress\n                        \n                    \n                    \n                            Longitudinal survey study\n                        \n                    \n                    \n                            Random-intercept cross-lagged panel model\n                        \n                    \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF of article\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Online version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data and code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Borghi, O., Mayrhofer, L., Voracek, M., & Tran, U. S. (2023). Differential associations of the two higher-order factors of mindfulness with trait empathy and the mediating role of emotional awareness. Scientific Reports, 13(1), Article 1. https://doi.org/10.1038/s41598-023-30323-6\n            \n\n            \n            \n                \n                    \n                            Mindfulness\n                        \n                    \n                    \n                            Empathy\n                        \n                    \n                    \n                            Observational study\n                        \n                    \n                    \n                            Structural equation modeling\n                        \n                    \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF of article\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Online version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data and code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#posters",
    "href": "research/index.html#posters",
    "title": "Research",
    "section": "Posters",
    "text": "Posters\n\n\n\n    \n        \n            \n                Poster of my master thesis.\n            \n\n            \n            \n                \n                    \n                            fMRI\n                        \n                    \n                    \n                            Action observation network\n                        \n                    \n                    \n                            Linear mixed modeling\n                        \n                    \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF of poster\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data and code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/articles/borghi-etal-2023/index.html#important-links",
    "href": "research/articles/borghi-etal-2023/index.html#important-links",
    "title": "Differential associations of the two higher-order factors of mindfulness with trait empathy and the mediating role of emotional awareness",
    "section": "Important links",
    "text": "Important links\n\nPaper\nOSF repository"
  },
  {
    "objectID": "research/articles/borghi-etal-2023/index.html#abstract",
    "href": "research/articles/borghi-etal-2023/index.html#abstract",
    "title": "Differential associations of the two higher-order factors of mindfulness with trait empathy and the mediating role of emotional awareness",
    "section": "Abstract",
    "text": "Abstract\nEmpathy enables us to understand the emotions of others and is an important determinant of prosocial behavior. Investigating the relationship between mindfulness and empathy could therefore provide important insights into factors that promote interpersonal understanding and pathways that contribute to prosocial behavior. As prior studies have yielded only inconsistent results, this study extended previous findings and investigated for the first time the associations of two important factors of mindfulness (Self-regulated Attention [SRA] and Orientation to Experience [OTE]) with two commonly proposed components of empathy (cognitive empathy and affective empathy). Using a community sample of N = 552 German-speaking adults, the two mindfulness factors were differentially associated with cognitive and affective empathy. SRA correlated positively with cognitive empathy (r = 0.44; OTE: r = 0.09), but OTE correlated negatively with affective empathy (r = −0.27; SRA: r = 0.11). This negative association was strongest for one specific aspect of affective empathy, emotional contagion. Revisiting previously reported mediating effects of emotion regulation, we found that emotional awareness mediated the associations with both components of empathy, but only for SRA. Together, these findings imply that mindfulness benefits the cognitive understanding of others’ emotions via two distinct pathways: by promoting emotional awareness (SRA) and by limiting the undue impact of others’ emotions on oneself (OTE)."
  },
  {
    "objectID": "research/articles/borghi-etal-2023/index.html#citation",
    "href": "research/articles/borghi-etal-2023/index.html#citation",
    "title": "Differential associations of the two higher-order factors of mindfulness with trait empathy and the mediating role of emotional awareness",
    "section": "Citation",
    "text": "Citation\n@article{borghiDifferentialAssociationsTwo2023,\n  title = {Differential Associations of the Two Higher-Order Factors of Mindfulness with Trait Empathy and the Mediating Role of Emotional Awareness},\n  author = {Borghi, Olaf and Mayrhofer, Lukas and Voracek, Martin and Tran, Ulrich S.},\n  year = {2023},\n  month = feb,\n  journal = {Scientific Reports},\n  volume = {13},\n  number = {1},\n  pages = {3201},\n  publisher = {Nature Publishing Group},\n  issn = {2045-2322},\n  doi = {10.1038/s41598-023-30323-6},\n  urldate = {2023-02-24},\n  copyright = {2023 The Author(s)},\n  langid = {english},\n  keywords = {Human behaviour,Psychology},\n}"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "blog/2024/04/16/index.html",
    "href": "blog/2024/04/16/index.html",
    "title": "Teaching Myself: Bayesian Path Modeling in blavaan and brms",
    "section": "",
    "text": "# use groundhog to make code maximally reproducible\nif (!require(\"groundhog\", quietly = TRUE)) {\n  install.packages(\"groundhog\")\n}\n## Attached: 'Groundhog' (Version: 3.1.2)\n## Tips and troubleshooting: https://groundhogR.com\nlibrary(\"groundhog\")\n\n# use groundhog to install and load packages - we may not use all of them and some consider it bad practise\n# to still load them, but all of them are nice to have and could be useful\npkgs &lt;- c(\"here\",         # System path management\n          \"tidyverse\",    # ggplot, dplyr, %&gt;%, and friends\n          \"easystats\",    # bayestestR, performance, effectsize and friends\n          \"modelsummary\", # Data and model summaries with tables and plots\n          \"psych\",        # Descriptive statistics and other tools\n          \"brms\",         # Bayesian modeling through Stan\n          \"blavaan\",      # Bayesian SEM\n          \"lavaan\",       # Frequentist SEM\n          \"semPlot\",      # Plots for SEM\n          \"tidybayes\",    # Integration of Bayesian models in tidy + ggplot workflow\n          \"broom\",        # Convert model objects to data frames\n          \"broom.mixed\",  # Convert brms model objects to data frames\n          \"tinytable\",    # Lightweight package to create tables\n          \"hrbrthemes\",   # Additional ggplot themes\n          \"extrafont\",    # Additional fonts for plots etc\n          \"ggdist\",       # Special geoms for posterior distributions\n          \"ggrepel\",      # Automatically position labels\n          \"gghalves\",     # Special half geoms\n          \"patchwork\"     # Combine ggplot objects\n          )\n\ngroundhog.library(pkgs, \"2024-03-01\") # change to a recent date\n## here() starts at C:/Users/Olaf/Documents/olafborghi.github.io/blog/2024/04/16\n## Warning: package 'ggplot2' was built under R version 4.3.3\n## ── Attaching core tidyverse packages ────────────────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.4     ✔ readr     2.1.5\n## ✔ forcats   1.0.0     ✔ stringr   1.5.1\n## ✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n## ✔ purrr     1.0.2\n## ── Conflicts ──────────────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n## Warning: package 'easystats' was built under R version 4.3.3\n## # Attaching packages: easystats 0.7.0 (red = needs update)\n## ✔ bayestestR  0.13.2   ✔ correlation 0.8.4 \n## ✖ datawizard  0.9.1    ✖ effectsize  0.8.6 \n## ✖ insight     0.19.8   ✔ modelbased  0.8.7 \n## ✖ performance 0.10.9   ✖ parameters  0.21.5\n## ✔ report      0.5.8    ✖ see         0.8.2 \n## \n## Restart the R-Session and update packages with `easystats::easystats_update()`.\n## \n## Version 2.0.0 of `modelsummary`, to be released soon, will introduce a breaking\n##   change: The default table-drawing package will be `tinytable` instead of\n##   `kableExtra`. All currently supported table-drawing packages will continue to be\n##   supported for the foreseeable future, including `kableExtra`, `gt`, `huxtable`,\n##   `flextable, and `DT`.\n##   \n##   You can always call the `config_modelsummary()` function to change the default\n##   table-drawing package in persistent fashion. To try `tinytable` now:\n##   \n##   config_modelsummary(factory_default = 'tinytable')\n##   \n##   To set the default back to `kableExtra`:\n##   \n##   config_modelsummary(factory_default = 'kableExtra')\n## \n## Attaching package: 'modelsummary'\n## \n## The following object is masked from 'package:parameters':\n## \n##     supported_models\n## \n## The following object is masked from 'package:insight':\n## \n##     supported_models\n## \n## \n## Attaching package: 'psych'\n## \n## The following object is masked from 'package:modelsummary':\n## \n##     SD\n## \n## The following object is masked from 'package:effectsize':\n## \n##     phi\n## \n## The following object is masked from 'package:datawizard':\n## \n##     rescale\n## \n## The following objects are masked from 'package:ggplot2':\n## \n##     %+%, alpha\n## \n## Loading required package: Rcpp\n## Loading 'brms' package (version 2.20.4). Useful instructions\n## can be found by typing help('brms'). A more detailed introduction\n## to the package is available through vignette('brms_overview').\n## \n## Attaching package: 'brms'\n## \n## The following object is masked from 'package:psych':\n## \n##     cs\n## \n## The following object is masked from 'package:stats':\n## \n##     ar\n## Warning: package 'blavaan' was built under R version 4.3.3\n## This is blavaan 0.5-3\n## On multicore systems, we suggest use of future::plan(\"multicore\") or\n##   future::plan(\"multisession\") for faster post-MCMC computations.\n## This is lavaan 0.6-17\n## lavaan is FREE software! Please report any bugs.\n## \n## Attaching package: 'lavaan'\n## \n## The following object is masked from 'package:psych':\n## \n##     cor2cov\n## \n## \n## Attaching package: 'tidybayes'\n## \n## The following objects are masked from 'package:brms':\n## \n##     dstudent_t, pstudent_t, qstudent_t, rstudent_t\n## \n## The following object is masked from 'package:parameters':\n## \n##     parameters\n## \n## The following object is masked from 'package:bayestestR':\n## \n##     hdi\n## Warning: package 'broom.mixed' was built under R version 4.3.3\n## NOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n##       Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n##       if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n## Registering fonts with R\n## \n## Attaching package: 'ggdist'\n## \n## The following objects are masked from 'package:brms':\n## \n##     dstudent_t, pstudent_t, qstudent_t, rstudent_t\n## \n## The following object is masked from 'package:bayestestR':\n## \n##     hdi\n## Warning: package 'ggrepel' was built under R version 4.3.3\n## Warning: package 'gghalves' was built under R version 4.3.3\n## Warning: package 'patchwork' was built under R version 4.3.3\n## [36mSuccessfully attached 'here_1.0.1'[0m\n## [36mSuccessfully attached 'tidyverse_2.0.0'[0m\n## [36mSuccessfully attached 'easystats_0.7.0'[0m\n## [36mSuccessfully attached 'modelsummary_1.4.5'[0m\n## [36mSuccessfully attached 'psych_2.4.1'[0m\n## [36mSuccessfully attached 'brms_2.20.4'[0m\n## [36mSuccessfully attached 'blavaan_0.5-3'[0m\n## [36mSuccessfully attached 'lavaan_0.6-17'[0m\n## [36mSuccessfully attached 'semPlot_1.1.6'[0m\n## [36mSuccessfully attached 'tidybayes_3.0.6'[0m\n## [36mSuccessfully attached 'broom_1.0.5'[0m\n## [36mSuccessfully attached 'broom.mixed_0.2.9.4'[0m\n## [36mSuccessfully attached 'tinytable_0.0.5'[0m\n## [36mSuccessfully attached 'hrbrthemes_0.8.0'[0m\n## [36mSuccessfully attached 'extrafont_0.19'[0m\n## [36mSuccessfully attached 'ggdist_3.3.1'[0m\n## [36mSuccessfully attached 'ggrepel_0.9.5'[0m\n## [36mSuccessfully attached 'gghalves_0.1.4'[0m\n## [36mSuccessfully attached 'patchwork_1.2.0'[0m"
  },
  {
    "objectID": "blog/2024/04/16/index.html#teaching-myself-bayesian-path-modeling",
    "href": "blog/2024/04/16/index.html#teaching-myself-bayesian-path-modeling",
    "title": "Teaching Myself: Bayesian Path Modeling in blavaan and brms",
    "section": "Teaching Myself: Bayesian Path Modeling",
    "text": "Teaching Myself: Bayesian Path Modeling\nUsing the same dataset as in previous blog posts, this time I will look into path models within a Bayesian statistical framework. Path models allow to directly check complex models, including associations between multiple predictors, multiple outcomes, and indirect associations.\nI will not touch upon the topics of causality etc. here. However, I want to point out that before running a path model, one should carefully think about the relationships of interest. In addition, analyses of indirect effects are often called “mediation analysis”. Some use mediation in a merely causal context, and as I don’t touch causality here, I will stick to the term “indirect association”. Path models on cross-sectional data are also criticized from time to time, so here a few suggested reads before you think of doing this in your own research:\n\nThat’s a Lot to Process! Pitfalls of Popular Path Models by Rohrer et al. (2022)\nThinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data, again by Rohrer (2018)\nAnd as many people criticize statistical models that test for indirect associations in cross-sectional data, here a bit of a different view on it.\n\nSo beware: This blog post is mainly about messing around with some data and trying out different functions!\nSpecifically, we want to investigate whether there is an indirect association between Age and Conservatism, Dogmatism, and Religiosity via five cognitive factors Caution, Temporal Discounting, Perceptual Processing Time, Speed of Evidence Accumulation, and Strategic Information Processing (Self-regulation ontology; see Eisenberg et al., 2018; 2019)\nMeet the data\nI will again use open data from Zmigrod et al. (2021). This dataset includes cognitive and personality factors derived from exploratory factor analysis (EFA) of a large number of behavioral cognitive tasks and personality questionnaires from the Self-Regulation Ontology and in addition to that, several measures of ideology.\nSome preparations and explorations\nExploratory factor analysis\nI will directly re-use some of the code that was shared together with the manuscript (Zmigrod et al., 2021; see above) to obtain three ideology factors using EFA on the scores of several questionnaires.\n\n# create df with relevant data for EFA \nefa_data &lt;- data.frame(\n        secs_social = data$SocialConservatism, \n        secs_econ = data$EconConservatism,\n        nation = data$Nationalism, \n        auth = data$Authoritarianism,\n        SJ = data$SJ, \n        sdo = data$SDO,\n        patriot = data$Patriotism,\n        progrSum = data$IngroupAttitude, #\n        dogma = data$Dogmatism,\n        IH1 = -1*data$IH1, # IH factors reversed because conceptually inverse of dogmatism\n        IH2 = -1*data$IH2,\n        IH3 = -1*data$IH3,\n        IH4 = -1*data$IH4,\n        rel_atten = data$ReligiousAttendance,\n        rel_pray = data$ReligiousPrayer,\n        rel_imp = data$ReligionImportance\n      )\n\n# run EFA \nscree(efa_data) # scree plot\n\n\n\n\n\n\nfa.parallel(efa_data, fm = \"ml\") # parallel analysis\n\n\n\n\n\n\n## Parallel analysis suggests that the number of factors =  3  and the number of components =  3\nIdeol_EFA &lt;- fa(efa_data, \n                nfactors=3, \n                rotate=\"oblimin\")     # EFA oblimin for 3 factors\n## Loading required namespace: GPArotation\n\nprint.psych(Ideol_EFA)\n## Factor Analysis using method =  minres\n## Call: fa(r = efa_data, nfactors = 3, rotate = \"oblimin\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##               MR1   MR2   MR3   h2    u2 com\n## secs_social  0.72 -0.06  0.32 0.76 0.245 1.4\n## secs_econ    0.75 -0.08 -0.06 0.51 0.488 1.0\n## nation       0.79  0.18 -0.02 0.70 0.299 1.1\n## auth         0.44  0.11  0.10 0.28 0.725 1.2\n## SJ           0.73 -0.02 -0.08 0.49 0.507 1.0\n## sdo          0.59  0.25 -0.13 0.44 0.564 1.5\n## patriot      0.76 -0.14  0.06 0.59 0.414 1.1\n## progrSum     0.53  0.14 -0.02 0.33 0.668 1.1\n## dogma        0.23  0.75  0.05 0.71 0.294 1.2\n## IH1         -0.17  0.54  0.13 0.29 0.705 1.3\n## IH2          0.01  0.62  0.08 0.40 0.604 1.0\n## IH3         -0.12  0.83 -0.07 0.66 0.342 1.1\n## IH4          0.07  0.63  0.06 0.44 0.561 1.0\n## rel_atten    0.07  0.04  0.72 0.56 0.440 1.0\n## rel_pray     0.02 -0.02  0.88 0.78 0.215 1.0\n## rel_imp     -0.04  0.03  0.99 0.95 0.047 1.0\n## \n##                        MR1  MR2  MR3\n## SS loadings           3.84 2.54 2.50\n## Proportion Var        0.24 0.16 0.16\n## Cumulative Var        0.24 0.40 0.56\n## Proportion Explained  0.43 0.29 0.28\n## Cumulative Proportion 0.43 0.72 1.00\n## \n##  With factor correlations of \n##      MR1  MR2  MR3\n## MR1 1.00 0.23 0.34\n## MR2 0.23 1.00 0.08\n## MR3 0.34 0.08 1.00\n## \n## Mean item complexity =  1.1\n## Test of the hypothesis that 3 factors are sufficient.\n## \n## df null model =  120  with the objective function =  8.8 with Chi Square =  2876\n## df of  the model are 75  and the objective function was  0.92 \n## \n## The root mean square of the residuals (RMSR) is  0.04 \n## The df corrected root mean square of the residuals is  0.05 \n## \n## The harmonic n.obs is  331 with the empirical chi square  117  with prob &lt;  0.0013 \n## The total n.obs was  334  with Likelihood Chi Square =  300  with prob &lt;  1.1e-28 \n## \n## Tucker Lewis Index of factoring reliability =  0.868\n## RMSEA index =  0.095  and the 90 % confidence intervals are  0.084 0.106\n## BIC =  -136\n## Fit based upon off diagonal values = 0.99\n## Measures of factor score adequacy             \n##                                                    MR1  MR2  MR3\n## Correlation of (regression) scores with factors   0.95 0.93 0.98\n## Multiple R square of scores with factors          0.91 0.86 0.97\n## Minimum correlation of possible factor scores     0.82 0.73 0.93\n\nAlright, just as in the manuscript the EFA indicates three factors, the first appears to be a factor of political conservatism, as there are high loadings of social and economic conservatism, nationalism, religiosity on this factor. The second appears to be a factor of dogmatism, with high loadings of the dogmatism and intellectual humility measure. The third appears to be a religiosity factor.\nIf you want to explore EFA a bit more, here again a suggested read Factor Analysis with the psych package by Micheal Clark. For now we don’t really care that much about the details, all we want are the three factor scores so that we can explore a few path models.\n\n# obtain factor loadings and add them to data\nfactor_scores &lt;- as.data.frame(Ideol_EFA$scores)\n\ndata &lt;- add_column(\n  data, \n  conservatism_factor = factor_scores$MR1, \n  dogmatism_factor = factor_scores$MR2, \n  religiosity_factor = factor_scores$MR3  \n  )\n\nStandardizing the data\nAgain, I will standardize the data. The scales are arbitrary anyway in this case, and this simplifies specifying the priors in this very basic example.\n\ndf &lt;- data %&gt;%\n  select(1, 3:11, 59:61) %&gt;%\n  rename(ID = SubjectN) %&gt;% \n  mutate(Edu = as.factor(Edu),\n         Gender = as.factor(Gender),\n         ID = as.factor(ID)) %&gt;% \n  mutate(across(where(is.numeric), scale))\n\nstr(df)\n## tibble [334 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ ID                 : Factor w/ 334 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Age                : num [1:334, 1] -1.776 -0.479 -1.068 -0.715 -0.951 ...\n##   ..- attr(*, \"scaled:center\")= num 37.1\n##   ..- attr(*, \"scaled:scale\")= num 8.49\n##  $ Edu                : Factor w/ 7 levels \"1\",\"2\",\"3\",\"4\",..: 4 3 3 1 5 7 5 5 4 4 ...\n##  $ Gender             : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 2 2 2 2 ...\n##  $ Income             : num [1:334, 1] 0.259 0.259 -0.666 -0.666 1.184 ...\n##   ..- attr(*, \"scaled:center\")= num 2.72\n##   ..- attr(*, \"scaled:scale\")= num 1.08\n##  $ SpeededIP          : num [1:334, 1] -1.844 -2.424 -1.974 -1.407 -0.603 ...\n##   ..- attr(*, \"scaled:center\")= num 0.00825\n##   ..- attr(*, \"scaled:scale\")= num 0.969\n##  $ StrategicIP        : num [1:334, 1] -1.469 -1.599 0.751 -1.028 -0.522 ...\n##   ..- attr(*, \"scaled:center\")= num -0.0448\n##   ..- attr(*, \"scaled:scale\")= num 1.04\n##  $ Percep             : num [1:334, 1] -0.0923 -0.6635 1.0988 -0.6263 0.2969 ...\n##   ..- attr(*, \"scaled:center\")= num 0.0484\n##   ..- attr(*, \"scaled:scale\")= num 1.02\n##  $ Caution            : num [1:334, 1] 0.194 -0.592 -1.213 -0.057 1.131 ...\n##   ..- attr(*, \"scaled:center\")= num 0.0285\n##   ..- attr(*, \"scaled:scale\")= num 1.03\n##  $ Discount           : num [1:334, 1] 0.492 1.608 1.569 1.196 -0.186 ...\n##   ..- attr(*, \"scaled:center\")= num -0.0189\n##   ..- attr(*, \"scaled:scale\")= num 1.03\n##  $ conservatism_factor: num [1:334, 1] 0.359 1.793 -0.143 0.491 0.307 ...\n##   ..- attr(*, \"scaled:center\")= num -0.0198\n##   ..- attr(*, \"scaled:scale\")= num 0.937\n##  $ dogmatism_factor   : num [1:334, 1] 2.025 0.584 -1.742 1.494 -0.424 ...\n##   ..- attr(*, \"scaled:center\")= num -0.0148\n##   ..- attr(*, \"scaled:scale\")= num 0.927\n##  $ religiosity_factor : num [1:334, 1] -0.72 1.81 -0.582 0.061 1.824 ...\n##   ..- attr(*, \"scaled:center\")= num 0.00144\n##   ..- attr(*, \"scaled:scale\")= num 0.986\n\nDescriptive analyses\nI recently learned about the modelsummary package, and it looks very very nice. So let’s try to create a descriptive table using the package.\n\ndf %&gt;% \n  select(Age, Income, SpeededIP, StrategicIP, Percep, Caution, Discount, conservatism_factor, dogmatism_factor, religiosity_factor) %&gt;% \n  datasummary_skim(.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnique\nMissing Pct.\nMean\nSD\nMin\nMedian\nMax\n\n\n\n\nAge\n42\n1\n−0.0\n1.0\n−1.8\n−0.1\n3.1\n\n\n\nIncome\n6\n2\n−0.0\n1.0\n−1.6\n0.3\n2.1\n\n\n\nSpeededIP\n321\n0\n0.0\n1.0\n−2.4\n−0.0\n2.7\n\n\n\nStrategicIP\n325\n0\n−0.0\n1.0\n−2.4\n−0.1\n2.6\n\n\n\nPercep\n315\n0\n−0.0\n1.0\n−2.6\n−0.0\n3.1\n\n\n\nCaution\n316\n0\n−0.0\n1.0\n−2.4\n0.0\n2.6\n\n\n\nDiscount\n319\n0\n0.0\n1.0\n−2.4\n0.1\n1.9\n\n\n\nconservatism_factor\n317\n5\n−0.0\n1.0\n−2.0\n−0.0\n2.6\n\n\n\ndogmatism_factor\n317\n5\n0.0\n1.0\n−2.1\n−0.1\n4.6\n\n\n\nreligiosity_factor\n317\n5\n−0.0\n1.0\n−0.9\n−0.6\n2.8\n\n\n\n\n\n\nOk, this is actually super super cool! We can also summarize the categorical variables. By default datasummary_skim will only summarize numerical variables (above I selected a few specific variables of interest on purpose). We can also tell it to just summarize categorical variables:\n\ndatasummary_skim(df, type = \"categorical\")\n## Warning: These variables were omitted because they include more than 50 levels: ID.\n\n\n\n\n\nN\n%\n\n\n\nEdu\n1\n2\n0.6\n\n\n\n2\n56\n16.8\n\n\n\n3\n69\n20.7\n\n\n\n4\n48\n14.4\n\n\n\n5\n125\n37.4\n\n\n\n6\n24\n7.2\n\n\n\n7\n8\n2.4\n\n\nGender\n0\n163\n48.8\n\n\n\n1\n167\n50.0\n\n\n\n\n\nAgain, this is so so cool! I am a big fan of the package already! Let’s see if we can find some more use for it a bit later."
  },
  {
    "objectID": "blog/2024/04/16/index.html#ready-for-the-path-models",
    "href": "blog/2024/04/16/index.html#ready-for-the-path-models",
    "title": "Teaching Myself: Bayesian Path Modeling in blavaan and brms",
    "section": "Ready for the Path Models?",
    "text": "Ready for the Path Models?\nUsing lavaan for frequentist path models\nPath models can also be seen as structural equation models with only observed and no latent variables. They do not model measurement error in a way as latent variable models do, but are thus usually also a bit less complex. We will first run a simple path model in lavaan, and then try to replicate the findings in brms and blavaan. Also, for now we will not control for demographic variables.\nLet’s specify our lavaan model.\n\nmodel1 &lt;- \n' \n# direct associations (outcome ~ direct predictor)\n    conservatism_factor ~ c1*Age\n    dogmatism_factor ~ c2*Age\n    religiosity_factor ~ c3*Age\n  \n# indirect associations (indirect predictor ~ direct predictor)\n    SpeededIP ~ a1*Age\n    StrategicIP ~ a2*Age\n    Percep ~ a3*Age\n    Caution ~ a4*Age\n    Discount ~ a5*Age\n    \n# indirect associations (outcome ~ indirect predictor)\n    conservatism_factor ~ b11*SpeededIP + b12*StrategicIP + b13*Percep + b14*Caution + b15*Discount\n    dogmatism_factor ~ b21*SpeededIP + b22*StrategicIP + b23*Percep + b24*Caution + b25*Discount\n    religiosity_factor ~ b31*SpeededIP + b32*StrategicIP + b33*Percep + b34*Caution + b35*Discount\n    \n# residual covariances\n\n# mediator residual covariance\n    SpeededIP ~~ StrategicIP + Percep + Caution + Discount\n    StrategicIP ~~ Percep + Caution + Discount\n    Percep ~~ Caution + Discount\n    Caution ~~ Discount\n\n# dependent residual covariance\n    conservatism_factor ~~ dogmatism_factor + religiosity_factor\n    dogmatism_factor ~~ religiosity_factor\n    \n# effect decomposition\n# x = age, m1 = SpeededIP, m2 = StrategicIP, m3 = Percep, m4 = Caution, m5 = Discount\n  \n# y1 = conservatism_factor;  \n    ind_x_m1_y1 := a1*b11\n    ind_x_m2_y1 := a2*b12\n    ind_x_m3_y1 := a3*b13\n    ind_x_m4_y1 := a4*b14\n    ind_x_m5_y1 := a4*b15\n    ind_x_y1 := ind_x_m1_y1 + ind_x_m2_y1 + ind_x_m3_y1 + ind_x_m4_y1 + ind_x_m5_y1\n    tot_x_y1 := ind_x_y1 + c1\n    \n# y2 = dogmatism_factor\n    ind_x_m1_y2 := a1*b21\n    ind_x_m2_y2 := a2*b22\n    ind_x_m3_y2 := a3*b23\n    ind_x_m4_y2 := a4*b24\n    ind_x_m5_y2 := a4*b25\n    ind_x_y2 := ind_x_m1_y2 + ind_x_m2_y2 + ind_x_m3_y2 + ind_x_m4_y2 + ind_x_m5_y2\n    tot_x_y2 := ind_x_y2 + c2\n  \n# y3 = religiosity_factor\n    ind_x_m1_y3 := a1*b31\n    ind_x_m2_y3 := a2*b32\n    ind_x_m3_y3 := a3*b33\n    ind_x_m4_y3 := a4*b34\n    ind_x_m5_y3 := a4*b35\n    ind_x_y3 := ind_x_m1_y3 + ind_x_m2_y3 + ind_x_m3_y3 + ind_x_m4_y3 + ind_x_m5_y3\n    tot_x_y3 := ind_x_y3 + c3\n'\n\nLet’s fit and run this path model in lavaan.\n\nfit1 &lt;- sem(model = model1, data  = df,\n            std.ov = TRUE, mimic = \"Mplus\", \n            estimator = \"ML\", missing = \"fiml\") \n## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: 4 cases were deleted due to missing values in \n##        exogenous variable(s), while fixed.x = TRUE.\n\n\nparest1 &lt;- parameterestimates(fit1)\nparest1 %&gt;% \n  slice(55:75) %&gt;% \n  select(label, est, se, pvalue, ci.lower, ci.upper) %&gt;% \n  tt(.)\n\n \n\n  \n    \n\ntinytable_b7by6g5zkz7c537w6gxr\n\n\n      \n\nlabel\n                est\n                se\n                pvalue\n                ci.lower\n                ci.upper\n              \n\n\nind_x_m1_y1\n                  -0.01473\n                  0.0169\n                  3.85e-01\n                  -0.04794\n                   0.01847\n                \n\nind_x_m2_y1\n                   0.02956\n                  0.0137\n                  3.15e-02\n                   0.00262\n                   0.05650\n                \n\nind_x_m3_y1\n                   0.00765\n                  0.0119\n                  5.21e-01\n                  -0.01572\n                   0.03103\n                \n\nind_x_m4_y1\n                   0.04944\n                  0.0178\n                  5.36e-03\n                   0.01464\n                   0.08423\n                \n\nind_x_m5_y1\n                   0.03731\n                  0.0166\n                  2.46e-02\n                   0.00478\n                   0.06985\n                \n\nind_x_y1\n                   0.10923\n                  0.0369\n                  3.10e-03\n                   0.03685\n                   0.18161\n                \n\ntot_x_y1\n                   0.27269\n                  0.0585\n                  3.16e-06\n                   0.15801\n                   0.38738\n                \n\nind_x_m1_y2\n                  -0.04499\n                  0.0197\n                  2.21e-02\n                  -0.08351\n                  -0.00647\n                \n\nind_x_m2_y2\n                   0.01060\n                  0.0100\n                  2.91e-01\n                  -0.00907\n                   0.03027\n                \n\nind_x_m3_y2\n                  -0.00642\n                  0.0126\n                  6.12e-01\n                  -0.03122\n                   0.01837\n                \n\nind_x_m4_y2\n                   0.01187\n                  0.0161\n                  4.61e-01\n                  -0.01970\n                   0.04343\n                \n\nind_x_m5_y2\n                   0.01522\n                  0.0162\n                  3.48e-01\n                  -0.01654\n                   0.04698\n                \n\nind_x_y2\n                  -0.01373\n                  0.0347\n                  6.92e-01\n                  -0.08168\n                   0.05423\n                \n\ntot_x_y2\n                  -0.07333\n                  0.0595\n                  2.18e-01\n                  -0.19000\n                   0.04333\n                \n\nind_x_m1_y3\n                  -0.00921\n                  0.0177\n                  6.03e-01\n                  -0.04395\n                   0.02552\n                \n\nind_x_m2_y3\n                   0.02789\n                  0.0136\n                  4.04e-02\n                   0.00122\n                   0.05456\n                \n\nind_x_m3_y3\n                   0.01286\n                  0.0128\n                  3.16e-01\n                  -0.01228\n                   0.03800\n                \n\nind_x_m4_y3\n                   0.02405\n                  0.0165\n                  1.44e-01\n                  -0.00819\n                   0.05630\n                \n\nind_x_m5_y3\n                   0.00584\n                  0.0158\n                  7.12e-01\n                  -0.02515\n                   0.03683\n                \n\nind_x_y3\n                   0.06143\n                  0.0350\n                  7.94e-02\n                  -0.00720\n                   0.13006\n                \n\ntot_x_y3\n                   0.13190\n                  0.0593\n                  2.62e-02\n                   0.01562\n                   0.24817\n                \n\n\n\n\n    \n\n\nLet’s remind ourselves of what is what:\nx = age, m1 = SpeededIP, m2 = StrategicIP, m3 = Percep, m4 = Caution, m5 = Discount, y1 = conservatism, y2 = dogmatism, y3 = religiosity.\nThe model indicates\n\nSignificant, but very small positive indirect associations between Age and Conservatism via the cognitive factors StrategicIP, Caution, and Discount.\nNo significant indirect associations between Age, Dogmatism and Religiosity via cognitive factors.\n\nWe can also check a bit more information using summary.\n\nsummary(fit1)\n## lavaan 0.6.17 ended normally after 18 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of model parameters                        52\n## \n##                                                   Used       Total\n##   Number of observations                           330         334\n##   Number of missing patterns                         2            \n## \n## Model Test User Model:\n##                                                       \n##   Test statistic                                 0.000\n##   Degrees of freedom                                 0\n## \n## Parameter Estimates:\n## \n##   Standard errors                             Standard\n##   Information                                 Observed\n##   Observed information based on                Hessian\n## \n## Regressions:\n##                         Estimate  Std.Err  z-value  P(&gt;|z|)\n##   conservatism_factor ~                                    \n##     Age       (c1)         0.163    0.059    2.786    0.005\n##   dogmatism_factor ~                                       \n##     Age       (c2)        -0.060    0.063   -0.953    0.341\n##   religiosity_factor ~                                     \n##     Age       (c3)         0.070    0.062    1.138    0.255\n##   SpeededIP ~                                              \n##     Age       (a1)         0.287    0.053    5.436    0.000\n##   StrategicIP ~                                            \n##     Age       (a2)        -0.151    0.054   -2.779    0.005\n##   Percep ~                                                 \n##     Age       (a3)         0.216    0.054    4.024    0.000\n##   Caution ~                                                \n##     Age       (a4)         0.274    0.053    5.169    0.000\n##   Discount ~                                               \n##     Age       (a5)        -0.072    0.055   -1.305    0.192\n##   conservatism_factor ~                                    \n##     SpeeddIP (b11)        -0.051    0.058   -0.881    0.378\n##     StrtgcIP (b12)        -0.195    0.058   -3.394    0.001\n##     Percep   (b13)         0.035    0.054    0.650    0.516\n##     Caution  (b14)         0.181    0.055    3.305    0.001\n##     Discount (b15)         0.136    0.055    2.496    0.013\n##   dogmatism_factor ~                                       \n##     SpeeddIP (b21)        -0.157    0.062   -2.524    0.012\n##     StrtgcIP (b22)        -0.070    0.061   -1.142    0.253\n##     Percep   (b23)        -0.030    0.058   -0.512    0.609\n##     Caution  (b24)         0.043    0.058    0.744    0.457\n##     Discount (b25)         0.056    0.058    0.955    0.339\n##   religiosity_factor ~                                     \n##     SpeeddIP (b31)        -0.032    0.062   -0.522    0.602\n##     StrtgcIP (b32)        -0.184    0.061   -3.035    0.002\n##     Percep   (b33)         0.059    0.057    1.035    0.300\n##     Caution  (b34)         0.088    0.058    1.525    0.127\n##     Discount (b35)         0.021    0.058    0.370    0.711\n## \n## Covariances:\n##                          Estimate  Std.Err  z-value  P(&gt;|z|)\n##  .SpeededIP ~~                                              \n##    .StrategicIP             0.340    0.055    6.163    0.000\n##    .Percep                 -0.177    0.052   -3.393    0.001\n##    .Caution                 0.109    0.051    2.146    0.032\n##    .Discount               -0.167    0.053   -3.129    0.002\n##  .StrategicIP ~~                                            \n##    .Percep                 -0.089    0.053   -1.668    0.095\n##    .Caution                 0.051    0.052    0.985    0.325\n##    .Discount               -0.292    0.056   -5.180    0.000\n##  .Percep ~~                                                 \n##    .Caution                 0.080    0.052    1.542    0.123\n##    .Discount                0.035    0.053    0.652    0.514\n##  .Caution ~~                                                \n##    .Discount               -0.032    0.053   -0.608    0.543\n##  .conservatism_factor ~~                                    \n##    .dogmatism_fctr          0.235    0.052    4.540    0.000\n##    .religisty_fctr          0.270    0.052    5.198    0.000\n##  .dogmatism_factor ~~                                       \n##    .religisty_fctr          0.074    0.053    1.391    0.164\n## \n## Intercepts:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)\n##    .consrvtsm_fctr   -0.006    0.052   -0.111    0.912\n##    .dogmatism_fctr   -0.000    0.055   -0.001    0.999\n##    .religisty_fctr   -0.002    0.054   -0.038    0.970\n##    .SpeededIP         0.000    0.053    0.000    1.000\n##    .StrategicIP      -0.000    0.054   -0.000    1.000\n##    .Percep            0.000    0.054    0.000    1.000\n##    .Caution           0.000    0.053    0.000    1.000\n##    .Discount         -0.000    0.055   -0.000    1.000\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)\n##    .consrvtsm_fctr    0.833    0.067   12.530    0.000\n##    .dogmatism_fctr    0.947    0.076   12.530    0.000\n##    .religisty_fctr    0.928    0.074   12.530    0.000\n##    .SpeededIP         0.915    0.071   12.845    0.000\n##    .StrategicIP       0.974    0.076   12.845    0.000\n##    .Percep            0.950    0.074   12.845    0.000\n##    .Caution           0.922    0.072   12.845    0.000\n##    .Discount          0.992    0.077   12.845    0.000\n## \n## Defined Parameters:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)\n##     ind_x_m1_y1      -0.015    0.017   -0.870    0.385\n##     ind_x_m2_y1       0.030    0.014    2.150    0.032\n##     ind_x_m3_y1       0.008    0.012    0.642    0.521\n##     ind_x_m4_y1       0.049    0.018    2.785    0.005\n##     ind_x_m5_y1       0.037    0.017    2.248    0.025\n##     ind_x_y1          0.109    0.037    2.958    0.003\n##     tot_x_y1          0.273    0.059    4.660    0.000\n##     ind_x_m1_y2      -0.045    0.020   -2.289    0.022\n##     ind_x_m2_y2       0.011    0.010    1.057    0.291\n##     ind_x_m3_y2      -0.006    0.013   -0.508    0.612\n##     ind_x_m4_y2       0.012    0.016    0.737    0.461\n##     ind_x_m5_y2       0.015    0.016    0.939    0.348\n##     ind_x_y2         -0.014    0.035   -0.396    0.692\n##     tot_x_y2         -0.073    0.060   -1.232    0.218\n##     ind_x_m1_y3      -0.009    0.018   -0.520    0.603\n##     ind_x_m2_y3       0.028    0.014    2.050    0.040\n##     ind_x_m3_y3       0.013    0.013    1.003    0.316\n##     ind_x_m4_y3       0.024    0.016    1.462    0.144\n##     ind_x_m5_y3       0.006    0.016    0.369    0.712\n##     ind_x_y3          0.061    0.035    1.754    0.079\n##     tot_x_y3          0.132    0.059    2.223    0.026\n\nAlso note: Our model perfectly fits the data, but this is because the model is just-identified (i.e., there are 0 degrees of freedom): Every pair of variable is connected - our model is just a different way to describe the full data. This does not mean that the model syntax is incorrect - this is often the case in models with just observed variables and indirect associations.\nThere is much more in this output that can be of interest. But as we want to focus on the Bayesian analyses here, let’s move on for now.\nblavaan: The Bayesian counterpart of lavaan\nUsing very similar syntax as lavaan, blavaan allows us to fit Bayesian structural equation models (or path models as here).\nLet’s check the default priors of blavaan.\n\ndpriors()\n##                nu             alpha            lambda              beta             theta \n##    \"normal(0,32)\"    \"normal(0,10)\"    \"normal(0,10)\"    \"normal(0,10)\" \"gamma(1,.5)[sd]\" \n##               psi               rho             ibpsi               tau \n## \"gamma(1,.5)[sd]\"       \"beta(1,1)\" \"wishart(3,iden)\"   \"normal(0,1.5)\"\n\nContrary to brms, in many cases we can use these default priors. We can modify the prior distributions, and in particular, we want to specify the prior of the regressions \\(\\beta\\) as \\(N(0,1)\\), as we will later use this prior in brms. It should not matter to much, especilly for the other parameters, but this is also good to try out on how to specify priors.\n\nmydp &lt;- dpriors(beta=\"normal(0,1)\")\nmydp\n##                nu             alpha            lambda              beta             theta \n##    \"normal(0,32)\"    \"normal(0,10)\"    \"normal(0,10)\"     \"normal(0,1)\" \"gamma(1,.5)[sd]\" \n##               psi               rho             ibpsi               tau \n## \"gamma(1,.5)[sd]\"       \"beta(1,1)\" \"wishart(3,iden)\"   \"normal(0,1.5)\"\n\nLet’s try to fit the same model as previously.\n\nfit2 &lt;- bsem(model1, dp = mydp, data = df)\n## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: 4 cases were deleted due to missing values in \n##        exogenous variable(s), while fixed.x = TRUE.\n## Computing post-estimation metrics (including lvs if requested)...\n## Warning in tmpmat[lower.tri(tmpmat)] &lt;- lavpartable$est[covpars]: number of items to\n## replace is not a multiple of replacement length\n\n\nsummary(fit2)\n## blavaan 0.5.3 ended normally after 1000 iterations\n## \n##   Estimator                                      BAYES\n##   Optimization method                             MCMC\n##   Number of model parameters                        52\n## \n##                                                   Used       Total\n##   Number of observations                           330         334\n##   Number of missing patterns                         2            \n## \n##   Statistic                                 MargLogLik         PPP\n##   Value                                      -3662.530       0.518\n## \n## Parameter Estimates:\n## \n## \n## Regressions:\n##                         Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##   conservatism_factor ~                                                             \n##     Age       (c1)         0.162    0.059    0.048    0.279    1.000     normal(0,1)\n##   dogmatism_factor ~                                                                \n##     Age       (c2)        -0.061    0.065   -0.189    0.065    0.999     normal(0,1)\n##   religiosity_factor ~                                                              \n##     Age       (c3)         0.069    0.063   -0.054    0.191    1.000     normal(0,1)\n##   SpeededIP ~                                                                       \n##     Age       (a1)         0.284    0.051    0.183    0.389    0.999     normal(0,1)\n##   StrategicIP ~                                                                     \n##     Age       (a2)        -0.151    0.055   -0.261   -0.045    1.000     normal(0,1)\n##   Percep ~                                                                          \n##     Age       (a3)         0.211    0.052    0.108    0.315    1.000     normal(0,1)\n##   Caution ~                                                                         \n##     Age       (a4)         0.272    0.054    0.168    0.375    1.000     normal(0,1)\n##   Discount ~                                                                        \n##     Age       (a5)        -0.071    0.056   -0.177    0.039    0.999     normal(0,1)\n##   conservatism_factor ~                                                             \n##     SpeeddIP (b11)        -0.052    0.059   -0.167    0.065    1.000     normal(0,1)\n##     StrtgcIP (b12)        -0.197    0.059   -0.313   -0.083    1.001     normal(0,1)\n##     Percep   (b13)         0.036    0.056   -0.073    0.149    1.000     normal(0,1)\n##     Caution  (b14)         0.182    0.056    0.075    0.289    1.000     normal(0,1)\n##     Discount (b15)         0.135    0.054    0.030    0.246    1.000     normal(0,1)\n##   dogmatism_factor ~                                                                \n##     SpeeddIP (b21)        -0.158    0.066   -0.289   -0.028    1.000     normal(0,1)\n##     StrtgcIP (b22)        -0.072    0.063   -0.193    0.050    1.000     normal(0,1)\n##     Percep   (b23)        -0.030    0.060   -0.148    0.089    1.000     normal(0,1)\n##     Caution  (b24)         0.046    0.057   -0.067    0.153    1.000     normal(0,1)\n##     Discount (b25)         0.054    0.058   -0.057    0.167    1.000     normal(0,1)\n##   religiosity_factor ~                                                              \n##     SpeeddIP (b31)        -0.031    0.062   -0.151    0.094    1.000     normal(0,1)\n##     StrtgcIP (b32)        -0.184    0.063   -0.308   -0.061    0.999     normal(0,1)\n##     Percep   (b33)         0.059    0.060   -0.061    0.176    1.000     normal(0,1)\n##     Caution  (b34)         0.087    0.059   -0.025    0.204    0.999     normal(0,1)\n##     Discount (b35)         0.021    0.058   -0.095    0.131    1.000     normal(0,1)\n## \n## Covariances:\n##                          Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##  .SpeededIP ~~                                                                       \n##    .StrategicIP             0.341    0.056    0.238    0.456    1.000     lkj_corr(1)\n##    .Percep                 -0.176    0.051   -0.279   -0.081    0.999     lkj_corr(1)\n##    .Caution                 0.110    0.052    0.011    0.215    0.999     lkj_corr(1)\n##    .Discount               -0.167    0.054   -0.276   -0.063    1.000     lkj_corr(1)\n##  .StrategicIP ~~                                                                     \n##    .Percep                 -0.087    0.054   -0.196    0.017    1.000     lkj_corr(1)\n##    .Caution                 0.050    0.054   -0.056    0.156    1.000     lkj_corr(1)\n##    .Discount               -0.296    0.057   -0.416   -0.187    1.000     lkj_corr(1)\n##  .Percep ~~                                                                          \n##    .Caution                 0.079    0.053   -0.023    0.182    0.999     lkj_corr(1)\n##    .Discount                0.035    0.055   -0.074    0.143    0.999     lkj_corr(1)\n##  .Caution ~~                                                                         \n##    .Discount               -0.033    0.053   -0.135    0.069    0.999     lkj_corr(1)\n##  .conservatism_factor ~~                                                             \n##    .dogmatism_fctr          0.242    0.056    0.132    0.349    1.001     lkj_corr(1)\n##    .religisty_fctr          0.275    0.054    0.177    0.385    1.002     lkj_corr(1)\n##  .dogmatism_factor ~~                                                                \n##    .religisty_fctr          0.075    0.056   -0.035    0.187    1.001     lkj_corr(1)\n## \n## Intercepts:\n##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##    .consrvtsm_fctr   -0.003    0.054   -0.107    0.103    0.999    normal(0,10)\n##    .dogmatism_fctr    0.004    0.057   -0.105    0.116    0.999    normal(0,10)\n##    .religisty_fctr   -0.011    0.056   -0.122    0.094    1.000    normal(0,10)\n##    .SpeededIP         0.003    0.053   -0.099    0.109    1.000    normal(0,10)\n##    .StrategicIP       0.013    0.055   -0.096    0.119    1.000    normal(0,10)\n##    .Percep           -0.023    0.052   -0.124    0.083    1.000    normal(0,10)\n##    .Caution           0.001    0.053   -0.103    0.103    1.000    normal(0,10)\n##    .Discount         -0.000    0.055   -0.111    0.109    1.000    normal(0,10)\n## \n## Variances:\n##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##    .consrvtsm_fctr    0.867    0.071    0.737    1.020    1.000 gamma(1,.5)[sd]\n##    .dogmatism_fctr    0.987    0.080    0.841    1.155    1.001 gamma(1,.5)[sd]\n##    .religisty_fctr    0.943    0.074    0.807    1.098    1.000 gamma(1,.5)[sd]\n##    .SpeededIP         0.926    0.073    0.798    1.080    0.999 gamma(1,.5)[sd]\n##    .StrategicIP       0.995    0.080    0.853    1.163    1.000 gamma(1,.5)[sd]\n##    .Percep            0.941    0.075    0.802    1.097    0.999 gamma(1,.5)[sd]\n##    .Caution           0.953    0.074    0.821    1.114    0.999 gamma(1,.5)[sd]\n##    .Discount          1.026    0.082    0.875    1.198    1.000 gamma(1,.5)[sd]\n## \n## Defined Parameters:\n##                    Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n##     ind_x_m1_y1      -0.015    0.017   -0.049    0.019                         \n##     ind_x_m2_y1       0.030    0.014    0.001    0.058                         \n##     ind_x_m3_y1       0.008    0.012   -0.017    0.032                         \n##     ind_x_m4_y1       0.049    0.018    0.014    0.085                         \n##     ind_x_m5_y1       0.037    0.017    0.004    0.070                         \n##     ind_x_y1          0.109    0.038    0.034    0.183                         \n##     tot_x_y1          0.271    0.059    0.155    0.387                         \n##     ind_x_m1_y2      -0.045    0.021   -0.085   -0.004                         \n##     ind_x_m2_y2       0.011    0.011   -0.011    0.032                         \n##     ind_x_m3_y2      -0.006    0.013   -0.032    0.019                         \n##     ind_x_m4_y2       0.012    0.016   -0.019    0.044                         \n##     ind_x_m5_y2       0.015    0.017   -0.018    0.047                         \n##     ind_x_y2         -0.013    0.036   -0.084    0.058                         \n##     tot_x_y2         -0.074    0.062   -0.195    0.047                         \n##     ind_x_m1_y3      -0.009    0.018   -0.043    0.026                         \n##     ind_x_m2_y3       0.028    0.014   -0.001    0.056                         \n##     ind_x_m3_y3       0.012    0.013   -0.014    0.038                         \n##     ind_x_m4_y3       0.024    0.017   -0.010    0.058                         \n##     ind_x_m5_y3       0.006    0.016   -0.026    0.037                         \n##     ind_x_y3          0.061    0.036   -0.009    0.131                         \n##     tot_x_y3          0.130    0.060    0.012    0.248\n\nThe results are very similar.\nPath models in brms\nWe can do something very similar also using brms! brms is a very flexible and powerful packages that is very flexible with modeling approaches. I think blavaan is the more straight-forward choice as soon as you want to specifiy latent variable models, but for models with just observed variables brms may even be the better pick, even in the case of path models. Let’s try it out and compare the results.\nSelecting priors\nI already scaled all variables of interest. This will make it a bit simpler to select priors. I will use weakly informative priors for slope and intercept parameters again (see https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations). I do this merely to keep things simple and straightforward here - usually a few more thoughts should be placed in specifying your prior. The selected prior is \\(N(0,1)\\). Let’s plot to prior to see what kind of an effect it may have on our parameters.\n\n\ncolors &lt;- ipsum_pal()(1)\n\nprior(normal(0, 1)) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(.width = 0.683, p_limits = c(.001, .999), \n               slab_fill = colors[1], slab_alpha = 0.8) +\n  scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) +\n  labs(title = \"Prior N(0,1)\", x = \"Parameter Space\", y = \"\") +\n  coord_cartesian(xlim = c(-4, 4)) +\n  scale_x_continuous(breaks = seq(-4, 4, by = 1)) +\n  theme_ipsum() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) \n\n\n\n\n\n\n\nSetting up the model\nIn brms we have to specify our model a bit different than previously. However, in principle, we are doing nothing else than fitting a multivariate linear regression once again. The way we set up this model should thus remind both of what we specified in the previous blog post, and of the lavaan syntax from above.\n\n\nmodel2 &lt;- \n  # direct and indirect paths to ideological factors\n  bf(conservatism_factor ~ Age + SpeededIP + StrategicIP + Percep + Caution + Discount) + \n  bf(dogmatism_factor ~ Age + SpeededIP + StrategicIP + Percep + Caution + Discount) + \n  bf(religiosity_factor ~ Age + SpeededIP + StrategicIP + Percep + Caution + Discount) +\n  # paths from age to indirect predictors (cognitive factors)\n  bf(SpeededIP ~ Age) + \n  bf(StrategicIP ~ Age) + \n  bf(Percep ~ Age) + \n  bf(Caution ~ Age) + \n  bf(Discount ~ Age)\n\nLet’s check how our priors are set by default in brms.\n\nget_prior(model2 + set_rescor(FALSE), data = df)\n## Warning: Rows containing NAs were excluded from the model.\n##                    prior     class        coef group               resp dpar nlpar lb ub\n##                   (flat)         b                                                      \n##                   (flat) Intercept                                                      \n##                   (flat)         b                              Caution                 \n##                   (flat)         b         Age                  Caution                 \n##     student_t(3, 0, 2.5) Intercept                              Caution                 \n##     student_t(3, 0, 2.5)     sigma                              Caution             0   \n##                   (flat)         b                   conservatismfactor                 \n##                   (flat)         b         Age       conservatismfactor                 \n##                   (flat)         b     Caution       conservatismfactor                 \n##                   (flat)         b    Discount       conservatismfactor                 \n##                   (flat)         b      Percep       conservatismfactor                 \n##                   (flat)         b   SpeededIP       conservatismfactor                 \n##                   (flat)         b StrategicIP       conservatismfactor                 \n##     student_t(3, 0, 2.5) Intercept                   conservatismfactor                 \n##     student_t(3, 0, 2.5)     sigma                   conservatismfactor             0   \n##                   (flat)         b                             Discount                 \n##                   (flat)         b         Age                 Discount                 \n##   student_t(3, 0.1, 2.5) Intercept                             Discount                 \n##     student_t(3, 0, 2.5)     sigma                             Discount             0   \n##                   (flat)         b                      dogmatismfactor                 \n##                   (flat)         b         Age          dogmatismfactor                 \n##                   (flat)         b     Caution          dogmatismfactor                 \n##                   (flat)         b    Discount          dogmatismfactor                 \n##                   (flat)         b      Percep          dogmatismfactor                 \n##                   (flat)         b   SpeededIP          dogmatismfactor                 \n##                   (flat)         b StrategicIP          dogmatismfactor                 \n##     student_t(3, 0, 2.5) Intercept                      dogmatismfactor                 \n##     student_t(3, 0, 2.5)     sigma                      dogmatismfactor             0   \n##                   (flat)         b                               Percep                 \n##                   (flat)         b         Age                   Percep                 \n##  student_t(3, -0.1, 2.5) Intercept                               Percep                 \n##     student_t(3, 0, 2.5)     sigma                               Percep             0   \n##                   (flat)         b                    religiosityfactor                 \n##                   (flat)         b         Age        religiosityfactor                 \n##                   (flat)         b     Caution        religiosityfactor                 \n##                   (flat)         b    Discount        religiosityfactor                 \n##                   (flat)         b      Percep        religiosityfactor                 \n##                   (flat)         b   SpeededIP        religiosityfactor                 \n##                   (flat)         b StrategicIP        religiosityfactor                 \n##  student_t(3, -0.6, 2.5) Intercept                    religiosityfactor                 \n##     student_t(3, 0, 2.5)     sigma                    religiosityfactor             0   \n##                   (flat)         b                            SpeededIP                 \n##                   (flat)         b         Age                SpeededIP                 \n##     student_t(3, 0, 2.5) Intercept                            SpeededIP                 \n##     student_t(3, 0, 2.5)     sigma                            SpeededIP             0   \n##                   (flat)         b                          StrategicIP                 \n##                   (flat)         b         Age              StrategicIP                 \n##     student_t(3, 0, 2.5) Intercept                          StrategicIP                 \n##     student_t(3, 0, 2.5)     sigma                          StrategicIP             0   \n##        source\n##       default\n##       default\n##       default\n##  (vectorized)\n##       default\n##       default\n##       default\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##       default\n##       default\n##       default\n##  (vectorized)\n##       default\n##       default\n##       default\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##       default\n##       default\n##       default\n##  (vectorized)\n##       default\n##       default\n##       default\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##       default\n##       default\n##       default\n##  (vectorized)\n##       default\n##       default\n##       default\n##  (vectorized)\n##       default\n##       default\n\nBy default brms sets non-informative flat priors for our slope parameters. We thus have to change this, and can do so directly when fitting the model.\nFitting and summarising the first model\n\nm3 &lt;- brm(model2 + set_rescor(FALSE), \n          data = df, \n          prior = c(prior(normal(0, 1), class = \"Intercept\"), \n                    prior(normal(0, 1), class = \"b\")),\n          seed = bayes_seed,\n          file = \"02_fits/fit_m3\")\n\nWe get a warning message:\nWarning: Rows containing NAs were excluded from the model.\nWe are not happy with this, but for now we will deal with it later.\nCheck the results. As usual, we can also use summary(m1.1) or print(m1.1) to do so.\n\nsummary(m3, prob = 0.95)  # prob = 0.95 is the default\n##  Family: MV(gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian, gaussian) \n##   Links: mu = identity; sigma = identity\n##          mu = identity; sigma = identity\n##          mu = identity; sigma = identity\n##          mu = identity; sigma = identity\n##          mu = identity; sigma = identity\n##          mu = identity; sigma = identity\n##          mu = identity; sigma = identity\n##          mu = identity; sigma = identity \n## Formula: conservatism_factor ~ Age + SpeededIP + StrategicIP + Percep + Caution + Discount \n##          dogmatism_factor ~ Age + SpeededIP + StrategicIP + Percep + Caution + Discount \n##          religiosity_factor ~ Age + SpeededIP + StrategicIP + Percep + Caution + Discount \n##          SpeededIP ~ Age \n##          StrategicIP ~ Age \n##          Percep ~ Age \n##          Caution ~ Age \n##          Discount ~ Age \n##    Data: df (Number of observations: 314) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## conservatismfactor_Intercept      -0.00      0.05    -0.11     0.11 1.00     6337\n## dogmatismfactor_Intercept          0.00      0.06    -0.11     0.12 1.00     6525\n## religiosityfactor_Intercept       -0.01      0.05    -0.11     0.09 1.00     6099\n## SpeededIP_Intercept                0.01      0.05    -0.09     0.12 1.00     6149\n## StrategicIP_Intercept              0.01      0.06    -0.10     0.13 1.00     5718\n## Percep_Intercept                  -0.03      0.05    -0.13     0.08 1.00     6003\n## Caution_Intercept                  0.03      0.05    -0.08     0.14 1.00     5574\n## Discount_Intercept                 0.01      0.06    -0.10     0.12 1.00     5602\n## conservatismfactor_Age             0.16      0.06     0.05     0.29 1.00     4368\n## conservatismfactor_SpeededIP      -0.05      0.06    -0.17     0.06 1.00     4502\n## conservatismfactor_StrategicIP    -0.20      0.06    -0.32    -0.08 1.00     5018\n## conservatismfactor_Percep          0.04      0.06    -0.08     0.15 1.00     5278\n## conservatismfactor_Caution         0.18      0.06     0.07     0.29 1.00     5297\n## conservatismfactor_Discount        0.14      0.06     0.02     0.25 1.00     5710\n## dogmatismfactor_Age               -0.06      0.06    -0.18     0.06 1.00     3949\n## dogmatismfactor_SpeededIP         -0.16      0.06    -0.28    -0.03 1.00     4213\n## dogmatismfactor_StrategicIP       -0.07      0.06    -0.19     0.05 1.00     4326\n## dogmatismfactor_Percep            -0.03      0.06    -0.15     0.09 1.00     4723\n## dogmatismfactor_Caution            0.04      0.06    -0.07     0.16 1.00     5841\n## dogmatismfactor_Discount           0.05      0.06    -0.07     0.18 1.00     5344\n## religiosityfactor_Age              0.07      0.06    -0.05     0.19 1.00     4160\n## religiosityfactor_SpeededIP       -0.03      0.06    -0.15     0.09 1.00     3946\n## religiosityfactor_StrategicIP     -0.18      0.06    -0.31    -0.06 1.00     4377\n## religiosityfactor_Percep           0.06      0.06    -0.06     0.18 1.00     4796\n## religiosityfactor_Caution          0.08      0.06    -0.03     0.20 1.00     5670\n## religiosityfactor_Discount         0.02      0.06    -0.10     0.14 1.00     4846\n## SpeededIP_Age                      0.28      0.05     0.18     0.39 1.00     5559\n## StrategicIP_Age                   -0.14      0.06    -0.26    -0.03 1.00     7492\n## Percep_Age                         0.21      0.05     0.11     0.32 1.00     5614\n## Caution_Age                        0.30      0.06     0.18     0.41 1.00     6587\n## Discount_Age                      -0.08      0.06    -0.19     0.03 1.00     6507\n##                                Tail_ESS\n## conservatismfactor_Intercept       2623\n## dogmatismfactor_Intercept          2985\n## religiosityfactor_Intercept        2855\n## SpeededIP_Intercept                3067\n## StrategicIP_Intercept              2994\n## Percep_Intercept                   3038\n## Caution_Intercept                  3158\n## Discount_Intercept                 3195\n## conservatismfactor_Age             3269\n## conservatismfactor_SpeededIP       3487\n## conservatismfactor_StrategicIP     3386\n## conservatismfactor_Percep          3219\n## conservatismfactor_Caution         3159\n## conservatismfactor_Discount        3473\n## dogmatismfactor_Age                3397\n## dogmatismfactor_SpeededIP          3290\n## dogmatismfactor_StrategicIP        3359\n## dogmatismfactor_Percep             3091\n## dogmatismfactor_Caution            3100\n## dogmatismfactor_Discount           3030\n## religiosityfactor_Age              2947\n## religiosityfactor_SpeededIP        3058\n## religiosityfactor_StrategicIP      3375\n## religiosityfactor_Percep           3125\n## religiosityfactor_Caution          2985\n## religiosityfactor_Discount         3287\n## SpeededIP_Age                      3144\n## StrategicIP_Age                    2951\n## Percep_Age                         3037\n## Caution_Age                        2954\n## Discount_Age                       3069\n## \n## Family Specific Parameters: \n##                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_conservatismfactor     0.93      0.04     0.86     1.00 1.00     4946     3050\n## sigma_dogmatismfactor        0.99      0.04     0.91     1.07 1.00     5632     2831\n## sigma_religiosityfactor      0.97      0.04     0.90     1.05 1.00     6108     2799\n## sigma_SpeededIP              0.97      0.04     0.89     1.04 1.00     7028     3235\n## sigma_StrategicIP            0.99      0.04     0.92     1.08 1.00     6009     3152\n## sigma_Percep                 0.96      0.04     0.89     1.04 1.00     5479     3026\n## sigma_Caution                0.97      0.04     0.89     1.05 1.00     5749     3318\n## sigma_Discount               1.00      0.04     0.93     1.09 1.00     6932     2907\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nThis time, we need some trickery to compute the indirect effects. Let’s remind ourselves how we did this previously - what I write out below, is what we specified in the lavaan syntax.\nThe indirect effect in general is calculated by the multiplication of the path from the direct predictor to the indirect predictor (a)\n\nIn this case, this is the path from Age to SpeededIP, StrategicIP, Percep, Caution, Discount respectively\n\nwith the path from the indirect predictor to the outcome (b)\n\nIn this case, this is the path from SpeededIP, StrategicIP, Percep, Caution, Discount to conservatism_factor, dogmatism_factor, or religiosity_factor\n\nAnd the direct paths (c) are the paths from the direct predictor to the outcome\n\nIn this case, this is the path from Age to conservatism_factor, dogmatism_factor, or religiosity_factor\n\nThe overall indirect effect for one outcome is nothing but the sum of all indirect effects to that outcome.\nThe total effects are nothing but the sum of the indirect and direct effects.\nIn brms we get the posterior of all of those effects. We can use the posterior draws to calculate these effects - though some manual work is needed. Let’s first extract the draws.\n\nm3_draws &lt;- as_draws_df(m3)\nhead(m3_draws)\n## # A draws_df: 6 iterations, 1 chains, and 41 variables\n##   b_conservatismfactor_Intercept b_dogmatismfactor_Intercept\n## 1                          0.034                      0.0476\n## 2                         -0.073                      0.0472\n## 3                          0.067                     -0.0393\n## 4                         -0.057                      0.0448\n## 5                          0.011                     -0.0044\n## 6                         -0.015                      0.0076\n##   b_religiosityfactor_Intercept b_SpeededIP_Intercept b_StrategicIP_Intercept\n## 1                         0.089                0.0604                  0.0062\n## 2                        -0.102                0.0793                 -0.0450\n## 3                         0.061               -0.0360                  0.0651\n## 4                        -0.091                0.0584                 -0.0283\n## 5                         0.042                0.0086                  0.0081\n## 6                        -0.064                0.0171                  0.0299\n##   b_Percep_Intercept b_Caution_Intercept b_Discount_Intercept\n## 1             -0.041              0.0104               -0.032\n## 2             -0.090              0.0472               -0.047\n## 3              0.026              0.0121                0.065\n## 4             -0.091              0.0820               -0.047\n## 5             -0.029              0.0161               -0.045\n## 6             -0.024             -0.0012                0.057\n## # ... with 33 more variables\n## # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\nNow let’s try to calculate the indirect effects.\n\nm3_draws &lt;- m3_draws %&gt;% \n  mutate(\n    # Age -&gt; Conservatism\n    ind_Age_Spe_Con = b_SpeededIP_Age * b_conservatismfactor_SpeededIP,\n    ind_Age_Str_Con = b_StrategicIP_Age * b_conservatismfactor_StrategicIP,\n    ind_Age_Per_Con = b_Percep_Age * b_conservatismfactor_Percep,\n    ind_Age_Cau_Con = b_Caution_Age * b_conservatismfactor_Caution,\n    ind_Age_Dis_Con = b_Discount_Age * b_conservatismfactor_Discount,\n    ind_Age_Con = ind_Age_Spe_Con + ind_Age_Str_Con + ind_Age_Per_Con + ind_Age_Cau_Con + ind_Age_Dis_Con,\n    tot_Age_Con = ind_Age_Con + b_conservatismfactor_Age,\n    \n    # Age -&gt; Dogmatism\n    ind_Age_Spe_Dog = b_SpeededIP_Age * b_dogmatismfactor_SpeededIP,\n    ind_Age_Str_Dog = b_StrategicIP_Age * b_dogmatismfactor_StrategicIP,\n    ind_Age_Per_Dog = b_Percep_Age * b_dogmatismfactor_Percep,\n    ind_Age_Cau_Dog = b_Caution_Age * b_dogmatismfactor_Caution,\n    ind_Age_Dis_Dog = b_Discount_Age * b_dogmatismfactor_Discount,\n    ind_Age_Dog = ind_Age_Spe_Dog + ind_Age_Str_Dog + ind_Age_Per_Dog + ind_Age_Cau_Dog + ind_Age_Dis_Dog,\n    tot_Age_Dog = ind_Age_Dog + b_dogmatismfactor_Age,\n    \n    # Age -&gt; Religiosity\n    ind_Age_Spe_Rel = b_SpeededIP_Age * b_religiosityfactor_SpeededIP,\n    ind_Age_Str_Rel = b_StrategicIP_Age * b_religiosityfactor_StrategicIP,\n    ind_Age_Per_Rel = b_Percep_Age * b_religiosityfactor_Percep,\n    ind_Age_Cau_Rel = b_Caution_Age * b_religiosityfactor_Caution,\n    ind_Age_Dis_Rel = b_Discount_Age * b_religiosityfactor_Discount,\n    ind_Age_Rel = ind_Age_Spe_Rel + ind_Age_Str_Rel + ind_Age_Per_Rel + ind_Age_Cau_Rel + ind_Age_Dis_Rel,\n    tot_Age_Rel = ind_Age_Rel + b_religiosityfactor_Age\n    )\n\nUse the describe_posterior function to get some summary statistics of what we calculated.\n\nm3_draws %&gt;% \n  select(45:65) %&gt;% \n  describe_posterior(., \n                     centrality = \"median\",\n                     dispersion = FALSE,\n                     ci = 0.95,\n                     ci_method = \"hdi\",\n                     rope_range = c(-0.02, 0.02),\n                     rope_ci = 1) \n## Warning: Dropping 'draws_df' class as required metadata was removed.\n## Summary of Posterior Distribution\n## \n## Parameter       |    Median |         95% CI |     pd |          ROPE | % in ROPE\n## ---------------------------------------------------------------------------------\n## ind_Age_Spe_Con |     -0.01 | [-0.05,  0.02] | 80.58% | [-0.02, 0.02] |    61.68%\n## ind_Age_Str_Con |      0.03 | [ 0.00,  0.06] | 99.48% | [-0.02, 0.02] |    30.18%\n## ind_Age_Per_Con |  7.16e-03 | [-0.02,  0.03] | 73.52% | [-0.02, 0.02] |    83.62%\n## ind_Age_Cau_Con |      0.05 | [ 0.02,  0.09] | 99.90% | [-0.02, 0.02] |     3.15%\n## ind_Age_Dis_Con | -9.59e-03 | [-0.03,  0.00] | 91.27% | [-0.02, 0.02] |    83.33%\n## ind_Age_Con     |      0.06 | [ 0.00,  0.13] | 97.20% | [-0.02, 0.02] |     8.88%\n## tot_Age_Con     |      0.23 | [ 0.12,  0.34] |   100% | [-0.02, 0.02] |        0%\n## ind_Age_Spe_Dog |     -0.04 | [-0.09, -0.01] | 99.25% | [-0.02, 0.02] |    10.08%\n## ind_Age_Str_Dog |  9.17e-03 | [-0.01,  0.03] | 87.12% | [-0.02, 0.02] |    83.33%\n## ind_Age_Per_Dog | -5.80e-03 | [-0.04,  0.02] | 69.17% | [-0.02, 0.02] |    82.83%\n## ind_Age_Cau_Dog |      0.01 | [-0.02,  0.05] | 77.25% | [-0.02, 0.02] |    63.70%\n## ind_Age_Dis_Dog | -3.16e-03 | [-0.02,  0.01] | 76.20% | [-0.02, 0.02] |    96.75%\n## ind_Age_Dog     |     -0.03 | [-0.10,  0.03] | 83.00% | [-0.02, 0.02] |    30.88%\n## tot_Age_Dog     |     -0.09 | [-0.20,  0.01] | 95.38% | [-0.02, 0.02] |     7.45%\n## ind_Age_Spe_Rel | -8.77e-03 | [-0.04,  0.03] | 70.28% | [-0.02, 0.02] |    69.05%\n## ind_Age_Str_Rel |      0.02 | [ 0.00,  0.05] | 99.40% | [-0.02, 0.02] |    35.43%\n## ind_Age_Per_Rel |      0.01 | [-0.01,  0.04] | 84.92% | [-0.02, 0.02] |    72.17%\n## ind_Age_Cau_Rel |      0.02 | [-0.01,  0.06] | 93.10% | [-0.02, 0.02] |    39.05%\n## ind_Age_Dis_Rel | -9.98e-04 | [-0.02,  0.01] | 62.18% | [-0.02, 0.02] |    99.08%\n## ind_Age_Rel     |      0.05 | [-0.01,  0.12] | 94.35% | [-0.02, 0.02] |    13.50%\n## tot_Age_Rel     |      0.12 | [ 0.02,  0.23] | 98.88% | [-0.02, 0.02] |     2.50%\n\nAlrighty, let’s compare to what we obtained previously in lavaan\n\nparest1 %&gt;% \n  slice(55:75) %&gt;% \n  select(label, est, pvalue, ci.lower, ci.upper) %&gt;% \n  mutate(across(where(is.numeric), ~round(.x, 2))) %&gt;% \n  tt(.)\n\n \n\n  \n    \n\ntinytable_ns9inqvd7d69qhj3dmr4\n\n\n      \n\nlabel\n                est\n                pvalue\n                ci.lower\n                ci.upper\n              \n\n\nind_x_m1_y1\n                  -0.01\n                  0.38\n                  -0.05\n                   0.02\n                \n\nind_x_m2_y1\n                   0.03\n                  0.03\n                   0.00\n                   0.06\n                \n\nind_x_m3_y1\n                   0.01\n                  0.52\n                  -0.02\n                   0.03\n                \n\nind_x_m4_y1\n                   0.05\n                  0.01\n                   0.01\n                   0.08\n                \n\nind_x_m5_y1\n                   0.04\n                  0.02\n                   0.00\n                   0.07\n                \n\nind_x_y1\n                   0.11\n                  0.00\n                   0.04\n                   0.18\n                \n\ntot_x_y1\n                   0.27\n                  0.00\n                   0.16\n                   0.39\n                \n\nind_x_m1_y2\n                  -0.04\n                  0.02\n                  -0.08\n                  -0.01\n                \n\nind_x_m2_y2\n                   0.01\n                  0.29\n                  -0.01\n                   0.03\n                \n\nind_x_m3_y2\n                  -0.01\n                  0.61\n                  -0.03\n                   0.02\n                \n\nind_x_m4_y2\n                   0.01\n                  0.46\n                  -0.02\n                   0.04\n                \n\nind_x_m5_y2\n                   0.02\n                  0.35\n                  -0.02\n                   0.05\n                \n\nind_x_y2\n                  -0.01\n                  0.69\n                  -0.08\n                   0.05\n                \n\ntot_x_y2\n                  -0.07\n                  0.22\n                  -0.19\n                   0.04\n                \n\nind_x_m1_y3\n                  -0.01\n                  0.60\n                  -0.04\n                   0.03\n                \n\nind_x_m2_y3\n                   0.03\n                  0.04\n                   0.00\n                   0.05\n                \n\nind_x_m3_y3\n                   0.01\n                  0.32\n                  -0.01\n                   0.04\n                \n\nind_x_m4_y3\n                   0.02\n                  0.14\n                  -0.01\n                   0.06\n                \n\nind_x_m5_y3\n                   0.01\n                  0.71\n                  -0.03\n                   0.04\n                \n\nind_x_y3\n                   0.06\n                  0.08\n                  -0.01\n                   0.13\n                \n\ntot_x_y3\n                   0.13\n                  0.03\n                   0.02\n                   0.25\n                \n\n\n\n\n    \n\n\nThe results that we got are very very similar and lead to the same interpretation.\nHowever, before we interpret the estimates at all, it is best practice to check if our model converged properly. Let’s follow the steps of the WAMB checklist this time!"
  },
  {
    "objectID": "blog/2024/04/16/index.html#resources",
    "href": "blog/2024/04/16/index.html#resources",
    "title": "Teaching Myself: Bayesian Path Modeling in blavaan and brms",
    "section": "Resources",
    "text": "Resources\nI am not a statistician, so please take all of what I wrote and coded here with a lot of caution. However, there are many great resources out there if you want to delve deeper into the topic - I am just getting started with this, but here are a few potentially helpful links.\n\nDoing Bayesian Data Analysis in brms and the tidyverse: A brms and tidyverse version by Solomon Kurz of the classic book by Kruschke Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. \nPractical Bayes Part I and Practical Bayes Part II: Great resources to dealing and avoiding common problems in Bayesian models by Micheal Clark\nBayesian Basics: General overview of how to run Bayesian models in R\nbayestestR: Vignettes on basic Bayesian models and their interpretation\nStan Wiki: Prior Choice: Go to reference for selecting priors\nCourse Handouts for Bayesian Data Analysis Class by Mark Lai\n\nAnd there are of course many many more resources, this is just to get started with some."
  },
  {
    "objectID": "blog/2024/03/19/index.html",
    "href": "blog/2024/03/19/index.html",
    "title": "Teaching Myself: Bayesian Linear Regression",
    "section": "",
    "text": "# use groundhog to make code maximally reproducible\nif (!require(\"groundhog\", quietly = TRUE)) {\n  install.packages(\"groundhog\")\n}\nlibrary(\"groundhog\")\n\n# use groundhog to install and load packages\npkgs &lt;- c(\"here\",         # Path management\n          \"tidyverse\",    # ggplot, dplyr, %&gt;%, and friends\n          \"easystats\",    # bayestestR, performance, effectsize and friends\n          \"psych\",        # Descriptive statistics and other tools\n          \"brms\",         # Bayesian modeling through Stan\n          \"tidybayes\",    # Integration of Bayesian models in tidy + ggplot workflow\n          \"broom\",        # Convert model objects to data frames\n          \"broom.mixed\",  # Convert brms model objects to data frames\n          \"tinytable\",    # Lightweight package to create tables\n          \"hrbrthemes\",   # Additional ggplot themes\n          \"extrafont\",    # Additional fonts for plots etc\n          \"ggdist\",       # Special geoms for posterior distributions\n          \"ggrepel\",      # Automatically position labels\n          \"gghalves\",     # Special half geoms\n          \"patchwork\"     # Combine ggplot objects\n          )\n\ngroundhog.library(pkgs, \"2024-03-01\") # change to a recent date"
  },
  {
    "objectID": "blog/2024/03/19/index.html#teaching-myself-bayesian-linear-regression",
    "href": "blog/2024/03/19/index.html#teaching-myself-bayesian-linear-regression",
    "title": "Teaching Myself: Bayesian Linear Regression",
    "section": "Teaching Myself: Bayesian Linear Regression",
    "text": "Teaching Myself: Bayesian Linear Regression\nMeet the data\nI will use a large open data set described in Eisenberg et al. (2018; 2019), combined with data from Zmigrod et al. (2021). The data from Eisenberg and colleagues includes measures from a large number of questionnaires and cognitive tasks that tap into the domain of self regulation. The data from Zmigrod and colleagues includes data from several measures of ideology.\nThe goal will be to find out whether self-report and task measures of self-control predict authoritarianism.\nVariables of interest:\n\nSelf-reported self control: Brief Self-Control Scale\nBehavioral self-control: Accuracy in a Go / No-Go task, a cognitive task that requires a response to a frequent Go stimulus, such as pressing the space bar when a red square is displayed, and the inhibition of this response when a No-Go stimulus is displayed.\nSelf reported authoritarianism.\n\n\ndata1 &lt;- data1 %&gt;%\n  rename(\"SubjectID\" = \"...1\")\n\ndata &lt;- inner_join(data1, data2, by = \"SubjectID\")\n\n\n# select variables of interest\nd &lt;- data %&gt;%\n  select(c(SubjectID, BSCSSCON, GNGDRPIM, Authoritarianism)) %&gt;%\n  rename(ID = SubjectID,\n         bscs = BSCSSCON,\n         gonogo = GNGDRPIM,\n         aut = Authoritarianism) %&gt;%\n  mutate(bscs_z = scale(bscs),\n         gonogo_z = scale(gonogo),\n         aut_z = scale(aut))\n\ndescribe(d)\n##          vars   n   mean    sd median trimmed    mad   min    max  range  skew kurtosis\n## ID*         1 334 167.50 96.56 167.50  167.50 123.80  1.00 334.00 333.00  0.00    -1.21\n## bscs        2 334  46.07 10.30  47.00   46.33  11.86 18.00  65.00  47.00 -0.22    -0.53\n## gonogo      3 333   3.65  0.76   3.73    3.67   0.64  0.82   5.97   5.15 -0.11     1.18\n## aut         4 334   4.43  2.94   4.00    4.25   2.97  0.00  12.00  12.00  0.55    -0.05\n## bscs_z      5 334   0.00  1.00   0.09    0.02   1.15 -2.73   1.84   4.56 -0.22    -0.53\n## gonogo_z    6 333   0.00  1.00   0.10    0.03   0.85 -3.74   3.07   6.81 -0.11     1.18\n## aut_z       7 334   0.00  1.00  -0.15   -0.06   1.01 -1.51   2.58   4.08  0.55    -0.05\n##            se\n## ID*      5.28\n## bscs     0.56\n## gonogo   0.04\n## aut      0.16\n## bscs_z   0.05\n## gonogo_z 0.05\n## aut_z    0.05\n\n\n\nvars1_density&lt;- d %&gt;%\n  select(c(bscs_z, gonogo_z, aut_z)) %&gt;%\n  pivot_longer(., cols = c(bscs_z, gonogo_z, aut_z), \n               names_to = \"Variable\", values_to = \"Value\") %&gt;%\n  ggplot(aes(x = Value, fill = factor(Variable))) + \n  geom_density(aes(alpha = 0.6)) + \n  labs(x=\"Standardised values\", y=element_blank(),\n       title=\"Distributions of predictor and outcome variables\") +\n  scale_fill_ipsum() +\n  scale_color_ipsum() +\n  guides(color = \"none\", alpha = \"none\") +\n  theme_ipsum(plot_title_size = 14) + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\nvars1_density\n\n\n\n\n\n\n\nHave a first visual look of the association between the variables.\n\n\nggplot(d, aes(x = bscs_z, y = aut_z)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\") +\n  labs(x=\"Brief Self-Control Scale\", y=\"Authoritarianism\",\n       title=\"Association between self-control and authoritarianism\") +\n  scale_fill_ipsum() +\n  scale_color_ipsum() +\n  guides(color = \"none\", alpha = \"none\") +\n  theme_ipsum(plot_title_size = 12)\n\n\n\n\n\n\n\n\n\nggplot(d, aes(x = gonogo_z, y = aut_z)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\") +\n  labs(x=\"Go / No-Go Accuracy\", y=\"Authoritarianism\",\n       title=\"Association between Go / No-go accuracy and authoritarianism\") +\n  scale_fill_ipsum() +\n  scale_color_ipsum() +\n  guides(color = \"none\", alpha = \"none\") +\n  theme_ipsum(plot_title_size = 12)\n\n\n\n\n\n\n\n\n\nd %&gt;%\n  select(c(bscs_z, gonogo_z, aut_z)) %&gt;%\n  pairs.panels(.)\n\n\n\n\n\n\n\nA first Bayesian linear regression: One continuous predictor\nWe already displayed regression lines. So far, however, all of them were fit using the frequentist lm() function. We want to change that and go Bayesian here. Let’s start by investigating the association between self-reported self-control and authoritarianism.\nSelecting priors\nI already scaled all variables of interest. This will make it a bit simpler to select priors. I will use weakly informative priors for our slope and intercept parameters (see https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations). The selected prior is \\(N(0,1)\\).\nIt is also recommended to conduct a prior sensitivity analysis. This can be done for example by refitting the model with a different prior and by comparing the results. We will thus fit another model using \\(N(0,2)\\) as my prior. Let’s plot both of them to see what kind of an effect they may have on our parameters.\n\n# simulate some data\nx_values &lt;- seq(-6, 6, length.out = 1000)\ndists &lt;- data.frame(x = x_values,\n                    y1 = dnorm(x_values, mean = 0, sd = 1),\n                    y2 = dnorm(x_values, mean = 0, sd = 2))\n\n# colors\ncolors &lt;- ipsum_pal()(2)\n\n# plot for N(0,1) \np1 &lt;- ggplot(dists, aes(x = x, y = y1)) +\n  geom_area(aes(fill = \"N(0,1)\"), alpha = 0.8) +\n  scale_fill_manual(values = c(\"N(0,1)\" = colors[1])) +\n  labs(title = \"N(0,1)\", y = \"Density\", x = \"Value\") +\n  theme_ipsum() +\n  theme(legend.position = \"none\")\n\n# plot for N(0,2) \np2 &lt;- ggplot(dists, aes(x = x, y = y2)) +\n  geom_area(aes(fill = \"N(0,2)\"), alpha = 0.8) +\n  scale_fill_manual(values = c(\"N(0,2)\" = colors[2])) +\n  labs(title = \"N(0,2)\", y = \"Density\", x = \"Value\") +\n  theme_ipsum() +\n  theme(legend.position = \"none\")\n\n# Combine plots \np_combined &lt;- p1 + p2 + plot_layout(ncol = 2)\nprint(p_combined)\n\n\n\n\n\n\n\nWe can see that in the the plot for \\(N(0,1)\\), more density is around 0. As we expect our parameter values to be between 0 and 1, this gives more weight to parameter values closer to zero, and less weight to parameter values that are far from zero, compared to \\(N(0,2)\\) , where more probability mass is also on parameters that are a bit further from 0. If we would know that the association is large, and at the same time our sample size is low, we could think of using informative priors.\nHere we prefer the prior with less information - this prior also has a slight regularizing effect.\nLet’s check how our priors are set by default in brms.\n\nget_prior(aut_z ~ bscs_z, data = d)\n##                    prior     class   coef group resp dpar nlpar lb ub       source\n##                   (flat)         b                                         default\n##                   (flat)         b bscs_z                             (vectorized)\n##  student_t(3, -0.1, 2.5) Intercept                                         default\n##     student_t(3, 0, 2.5)     sigma                               0         default\n\nBy default brms sets non-informative flat priors for our slope parameters. We thus have to change this, and can do so directly when fitting the model.\nFitting and summarising the first model\n\nm1.1 &lt;- brm(aut_z ~ bscs_z, \n          data = d, \n          prior = c(prior(normal(0, 1), class = \"Intercept\"), \n                    prior(normal(0, 1), class = \"b\")),\n          seed = bayes_seed,\n          file = \"02_fits/fit_m1.1\")\n\nCheck the results. We can use either summary(m1.1) or print(m1.1) to do so.\n\nsummary(m1.1, prob = 0.95)  # prob = 0.95 is the default\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: aut_z ~ bscs_z \n##    Data: d (Number of observations: 334) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.00      0.05    -0.11     0.11 1.00     4111     2956\n## bscs_z        0.15      0.05     0.05     0.26 1.00     3702     2562\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.99      0.04     0.92     1.07 1.00     3605     3062\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nUse the tools from bayestestR to have another look at the posterior. We also define a region of practical equivalence ranging from [-0.05; 0.05].\nMore on this later.\n\nm1.1_posteriors &lt;- describe_posterior(m1.1, \n                                      centrality = \"median\",\n                                      dispersion = FALSE,\n                                      ci = 0.95,\n                                      ci_method = \"hdi\",\n                                      test = c(\"rope\"),\n                                      rope_range = c(-0.05, 0.05),\n                                      rope_ci = 1)\n\nm1.1_posteriors &lt;- as.data.frame(m1.1_posteriors)\nm1.1_posteriors\n##     Parameter   Median   CI  CI_low CI_high ROPE_CI ROPE_low ROPE_high ROPE_Percentage\n## 2 b_Intercept 0.000595 0.95 -0.1085   0.103       1    -0.05      0.05          0.6392\n## 1    b_bscs_z 0.153896 0.95  0.0424   0.253       1    -0.05      0.05          0.0272\n##    Rhat  ESS\n## 2 1.000 4139\n## 1 0.999 3690\n\nPlot the conditional effects.\n\nc_eff_m1.1 &lt;- conditional_effects(m1.1)\nc_eff_m1.1_plot &lt;- plot(c_eff_m1.1, \n                        points = T,\n                        point_args = list(size = 1, alpha = 1/4, width = .05, height = .05, color = \"black\"),\n                        plot = FALSE)[[1]] +\n  scale_color_ipsum() +\n  scale_fill_ipsum() +\n  theme_ipsum()\n  \nc_eff_m1.1_plot\n\n\n\n\n\n\n\nLet’s use bayesplot functions to plot the posterior distributions of the parameters.\n\n\nmcmc_plot(m1.1, \n          type = \"areas\") +\n  theme_ipsum()\n\n\n\n\n\n\n\nWe can also manually create a similar plot using draws from the posterior distribution. For simplicity, Here we use gather_draws from the tidybayes package - this automatically returns a long dataframe suitable for plotting in ggplot.\n\ndraws_m1.1 &lt;- m1.1 %&gt;% \n  gather_draws(`b_.*`, regex = TRUE)  %&gt;% \n  mutate(variable = case_when(.variable == \"b_Intercept\" ~ \"Intercept\",\n                              .variable == \"b_bscs_z\"    ~ \"Self-Control\",\n                              TRUE  ~ .variable # Keeps the original value if it doesn't match any of the above\n                              ))\n\nggplot(draws_m1.1, aes(x = .value, y = fct_rev(variable), fill = variable)) +\n  geom_rect(aes(xmin = -0.05, xmax = 0.05, ymin = -Inf, ymax = Inf), fill = \"#FFE3EB\", alpha = 0.05) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(slab_alpha = 0.8,\n               .width = c(0.9, 0.95), point_interval = \"median_hdi\") +\n  scale_fill_ipsum() +\n  guides(fill = \"none\", slab_alpha = \"none\", alpha = \"none\") +\n  labs(title = \"Effect estimates\", x = \"Coefficient\", y = \"Variable\",\n       caption = \"90% and 95% credible intervals shown in black. Area in rose indicates ROPE.\") +\n  theme_ipsum(plot_title_size = 14,\n              axis_title_size = 12,\n              axis_title_face = \"bold\")\n\n\n\n\n\n\n\nChecking the model\nPrior check\nLet’s check if the prior that we used for the slope and intercept was informative or, as intended, only weakly informative. We can use functionality from bayestestR for this.\n\n\nprior_summary(m1.1)\n##                 prior     class   coef group resp dpar nlpar lb ub       source\n##          normal(0, 1)         b                                            user\n##          normal(0, 1)         b bscs_z                             (vectorized)\n##          normal(0, 1) Intercept                                            user\n##  student_t(3, 0, 2.5)     sigma                               0         default\ncheck_prior(m1.1)\n##     Parameter Prior_Quality\n## 1 b_Intercept uninformative\n## 2    b_bscs_z uninformative\n\nOk - as intended our priors were uninformative.\nPosterior predictive check\nLet’s also check how well our model performs in generating data that is similar to our observed data. We call this a Posterior predictive check (PPC).\n\npp_check(m1.1, ndraws= 200)\n\n\n\n\n\n\n\n\npp_check(m1.1, ndraws = 100, type ='error_scatter_avg', alpha = .1)\n\n\n\n\n\n\n\nTrace plot\nCommonly, we look at the trace plots, i.e., the estimated values across each iteration for a chain. See this nice practical guide by Michael Clark for some details. We will follow his code in several steps below.\nWhat should the trace plot look like? Ideally it should look like a series from a normal distribution.\n\nqplot(x = 1:1000, y = rnorm(1000), geom = 'line') +\n  theme_ipsum()\n\n\n\n\n\n\n\nNow we can also plot the trace plot from our model, using nice functionality from bayesplot.\n\nmcmc_plot(m1.1, type = 'trace') +\n  theme_ipsum()\n\n\n\n\n\n\n\nPrior sensitivity checks\nOk, so we fit our first model. We also want to check whether the choice of different priors would have a substantial effect. We refit a similar model, just this time with \\(N(0,2)\\) as prior for the intercept and slope parameter.\n\nm1.2 &lt;- brm(aut_z ~ bscs_z, \n          data = d, \n          prior = c(prior(normal(0, 2), class = \"Intercept\"), \n                    prior(normal(0, 2), class = \"b\")),\n          seed = bayes_seed,\n          file = \"02_fits/fit_m1.2\")\n\n\nsummary(m1.1, prob = 0.95)  # prob = 0.95 is the default\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: aut_z ~ bscs_z \n##    Data: d (Number of observations: 334) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.00      0.05    -0.11     0.11 1.00     4111     2956\n## bscs_z        0.15      0.05     0.05     0.26 1.00     3702     2562\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.99      0.04     0.92     1.07 1.00     3605     3062\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nm1.2_posteriors &lt;- describe_posterior(m1.2, \n                                      centrality = \"median\",\n                                      dispersion = FALSE,\n                                      ci = 0.95,\n                                      ci_method = \"hdi\",\n                                      test = c(\"rope\"),\n                                      rope_range = c(-0.05, 0.05),\n                                      rope_ci = 1)\n\nm1.2_posteriors &lt;- as.data.frame(m1.2_posteriors)\n\n# Add a new column 'model' to each dataframe\nm1.1_posteriors &lt;- m1.1_posteriors %&gt;% mutate(model = \"m1.1\")\nm1.2_posteriors &lt;- m1.2_posteriors %&gt;% mutate(model = \"m1.2\")\n\n# Combine the dataframes\nm1.1_m1.2_combined &lt;- bind_rows(m1.1_posteriors, m1.2_posteriors)\n\n# Print the combined dataframe\nm1.1_m1.2_combined %&gt;% \n  select(Parameter, Median, CI_low, CI_high, model) %&gt;% \n  tt(.)\n\n \n\n  \n    \n\ntinytable_0omihsch5sn1oesd8yp3\n\n\n      \n\nParameter\n                Median\n                CI_low\n                CI_high\n                model\n              \n\n\nb_Intercept\n                  0.000595\n                  -0.1085\n                  0.103\n                  m1.1\n                \n\nb_bscs_z\n                  0.153896\n                   0.0424\n                  0.253\n                  m1.1\n                \n\nb_Intercept\n                  0.000932\n                  -0.1045\n                  0.109\n                  m1.2\n                \n\nb_bscs_z\n                  0.152366\n                   0.0470\n                  0.265\n                  m1.2\n                \n\n\n\n\n    \n\n\nWe can see that irrespective of our prior choice, both the median of the posterior distributions of the two different models, and the 95% HDI are very similar."
  },
  {
    "objectID": "blog/2024/03/19/index.html#takeaway",
    "href": "blog/2024/03/19/index.html#takeaway",
    "title": "Teaching Myself: Bayesian Linear Regression",
    "section": "Takeaway",
    "text": "Takeaway\n\nUsing the Bayesian approach we get much more than just a point estimate for our slope parameter of interest - we get a posterior distribution of parameters that are plausible given our observed data.\nAn increase of self control of 1 SD is associated with an increase of 0.04-0.26 SDs (median: 0.15) in authoritarianism.\nThe 95% HDI of our predictor of interest “Self-Control” does not include 0. We can thus say with a relatively high probability (according to common criteria - some authors also argue for 89% or 90% CIs) given our data that the effect is different from 0.\nThis is still true when we define and look at the ROPE - a region of practical equivalence. There is only minor overlap between the posterior distribution of our parameter and the ROPE (around 3%), indicating that the probability that our effect is practically relevant given our defined criteria is large."
  },
  {
    "objectID": "blog/2024/03/19/index.html#resources",
    "href": "blog/2024/03/19/index.html#resources",
    "title": "Teaching Myself: Bayesian Linear Regression",
    "section": "Resources",
    "text": "Resources\nI am not a statistician, so please take all of what I wrote and coded here with a lot of caution. However, there are many great resources out there if you want to delve deeper into the topic - I am just getting started with this, but here are a few potentially helpful links.\n\nDoing Bayesian Data Analysis in brms and the tidyverse: A brms and tidyverse version by Solomon Kurz of the classic book by Kruschke Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. \nPractical Bayes Part I and Practical Bayes Part II: Great resources to dealing and avoiding common problems in Bayesian models by Micheal Clark\nBayesian Basics: General overview of how to run Bayesian models in R\nbayestestR: Vignettes on basic Bayesian models and their interpretation\nStan Wiki: Prior Choice: Go to reference for selecting priors\nCourse Handouts for Bayesian Data Analysis Class by Mark Lai\n\nAnd there are of course many many more resources, this is just to get started with some."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About me",
    "section": "",
    "text": "I was born and grew up in South Tyrol, a small multilingual region in Northern Italy.\nIn 2018 I moved to Austria to study Psychology at the University of Vienna, where I completed a BSc in Psychology and a MSc in Psychology with a specialization in social neuroscience, cognitive psychology and research methods.\nIn my master thesis, I investigated factors that modulate neural activation during action observation. I also got to opportunity to work as a research assistant in the Social, Cognitive and Affective Neuroscience (SCAN) Unit, where I contributed to projects on comparative neuroscience with dogs and humans and where I found my passion for research under the guidance of my mentors and supervisors Magdalena Boch and Claus Lamm.\nIn September 2023, I got the great opportunity to move to London to join the IP-PAD Doctoral Network and to persue a PhD in Psychology at Royal Holloway, University of London, with Manos Tsakiris as my primary and Kaat Smets as secondary PhD supervisors. At Royal Holloway, I am closely collaborating with researchers from the Centre for the Politics of Feelings and the Lab of Action and Body, as well as with researchers from the Department of Politics and International Relations.\nYou can hear more about my research project and myself in the short video below.\n\n\n\n\nIP-PAD MSCA Doctoral Network Member"
  },
  {
    "objectID": "blog/2024/03/14/index.html",
    "href": "blog/2024/03/14/index.html",
    "title": "Hello world",
    "section": "",
    "text": "For now, this is just a placeholder. I plan to share my experiences, learning experiences, resources and tutorials here along my journey to obtain my PhD in a highly interdisciplinary research field. So stay tuned!"
  },
  {
    "objectID": "blog/2024/03/30/index.html",
    "href": "blog/2024/03/30/index.html",
    "title": "Teaching Myself: Bayesian Multiple and Multivariate Linear Regression",
    "section": "",
    "text": "# use groundhog to make code maximally reproducible\nif (!require(\"groundhog\", quietly = TRUE)) {\n  install.packages(\"groundhog\")\n}\nlibrary(\"groundhog\")\n\n# use groundhog to install and load packages - we may not use all of them and some consider it bad practise\n# to still load them, but all of them are nice to have and could be useful\npkgs &lt;- c(\"here\",         # Path management\n          \"tidyverse\",    # ggplot, dplyr, %&gt;%, and friends\n          \"easystats\",    # bayestestR, performance, effectsize and friends\n          \"psych\",        # Descriptive statistics and other tools\n          \"brms\",         # Bayesian modeling through Stan\n          \"tidybayes\",    # Integration of Bayesian models in tidy + ggplot workflow\n          \"broom\",        # Convert model objects to data frames\n          \"broom.mixed\",  # Convert brms model objects to data frames\n          \"marginaleffects\", # Compute marginal effects\n          \"tinytable\",    # Lightweight package to create tables\n          \"hrbrthemes\",   # Additional ggplot themes\n          \"extrafont\",    # Additional fonts for plots etc\n          \"ggdist\",       # Special geoms for posterior distributions\n          \"ggrepel\",      # Automatically position labels\n          \"gghalves\",     # Special half geoms\n          \"patchwork\"     # Combine ggplot objects\n          )\n\ngroundhog.library(pkgs, \"2024-03-01\") # change to a recent date"
  },
  {
    "objectID": "blog/2024/03/30/index.html#teaching-myself-bayesian-multiple-linear-regression",
    "href": "blog/2024/03/30/index.html#teaching-myself-bayesian-multiple-linear-regression",
    "title": "Teaching Myself: Bayesian Multiple and Multivariate Linear Regression",
    "section": "Teaching Myself: Bayesian Multiple Linear Regression",
    "text": "Teaching Myself: Bayesian Multiple Linear Regression\nThis blog article builds on the analyses from the last blog post on Bayesian linear regression. We will use part of the same data, but this time investigate the effects of multiple predictor variables simultaneously. You may have noticed how we plotted the association of self-reported and behavioral self-control with authoritarianism, but in our regression only included self-reported self-control as a predictor. So let’s go one step further!\nMeet the data\nThis time, I only use open data from Zmigrod et al. (2021). This dataset includes cognitive and personality factors derived from exploratory factor analysis (EFA) of a large number of behavioral cognitive tasks and personality questionnaires from the Self-Regulation Ontology, and in addition to that, several measures of ideology.\nZmigrod et al. (2021) use EFA to obtain three factors of ideology. I can highly suggest to read the article, both the analytic approach and the results are very interesting - You can find it here.\nIn this blog post I will skip the factor analysis of the ideology measures, and directly investigate the relative associations of the cognitive factors with self-reported measures of ideology, thus taking a bit of a different analytic approach.\nWe will investigate the associations of five cognitive factors\n\nCaution, Temporal Discounting, Perceptual Processing Time, Speed of Evidence Accumulation, and Strategic Information Processing\n\nWith the score on three ideology questionnaires that measure\n\nConservatism, Authoritarianism, and Dogmatism\n\nAnd we will control for the demographic variables\n\nGender, Age, Education and Income\n\nLet’s select the data that we are interested in here.\n\n# select variables of interest\ndat &lt;- data %&gt;%\n  select(1, 3:11, 24, 32, 33) %&gt;%\n  rename(ID = SubjectN) \n\nstr(dat)\n## tibble [334 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ ID              : num [1:334] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Age             : num [1:334] 22 33 28 31 29 37 38 35 43 39 ...\n##  $ Edu             : num [1:334] 4 3 3 1 5 7 5 5 4 4 ...\n##  $ Gender          : num [1:334] 0 0 0 0 1 0 1 1 1 1 ...\n##  $ Income          : num [1:334] 3 3 2 2 4 1 4 2 3 2 ...\n##  $ SpeededIP       : num [1:334] -1.779 -2.342 -1.905 -1.356 -0.576 ...\n##  $ StrategicIP     : num [1:334] -1.568 -1.703 0.734 -1.111 -0.586 ...\n##  $ Percep          : num [1:334] -0.046 -0.63 1.172 -0.592 0.352 ...\n##  $ Caution         : num [1:334] 0.228 -0.579 -1.217 -0.03 1.19 ...\n##  $ Discount        : num [1:334] 0.488 1.637 1.597 1.213 -0.21 ...\n##  $ Conservatism    : num [1:334] 67 120 79 82 101 62 63 69 84 75 ...\n##  $ Authoritarianism: num [1:334] 3 1 3 8 8 4 1 4 5 6 ...\n##  $ Dogmatism       : num [1:334] 44 38 17 41 29 42 11 23 28 19 ...\n\nI want to standardize the variables again. However, as we have categorical and binary predictors, to be able to compare the sizes of effects, this time we standardize all continuous variables by 2 SD. This puts them on a scale that is comparable with binary or dummy-coded predictors.\nFor more information on this procedure, please see these articles / blog posts by Andrew Gelman\nhttps://statmodeling.stat.columbia.edu/2009/07/11/when_to_standar/\nhttp://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf\nLet’s first change the data type of Gender.\n\n\nd &lt;- dat %&gt;% \n  mutate(Gender = as.factor(Gender))\n\nstr(d)\n## tibble [334 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ ID              : num [1:334] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ Age             : num [1:334] 22 33 28 31 29 37 38 35 43 39 ...\n##  $ Edu             : num [1:334] 4 3 3 1 5 7 5 5 4 4 ...\n##  $ Gender          : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 2 2 2 2 ...\n##  $ Income          : num [1:334] 3 3 2 2 4 1 4 2 3 2 ...\n##  $ SpeededIP       : num [1:334] -1.779 -2.342 -1.905 -1.356 -0.576 ...\n##  $ StrategicIP     : num [1:334] -1.568 -1.703 0.734 -1.111 -0.586 ...\n##  $ Percep          : num [1:334] -0.046 -0.63 1.172 -0.592 0.352 ...\n##  $ Caution         : num [1:334] 0.228 -0.579 -1.217 -0.03 1.19 ...\n##  $ Discount        : num [1:334] 0.488 1.637 1.597 1.213 -0.21 ...\n##  $ Conservatism    : num [1:334] 67 120 79 82 101 62 63 69 84 75 ...\n##  $ Authoritarianism: num [1:334] 3 1 3 8 8 4 1 4 5 6 ...\n##  $ Dogmatism       : num [1:334] 44 38 17 41 29 42 11 23 28 19 ...\n\nAlright, now let’s scale the continuous predictor variables to have a mean of 0 and a sd of 0.5 and the continuous outcome variables to have a mean of 0 and a SD of 1.\n\n\nd &lt;- d %&gt;% \n  mutate(across(c(SpeededIP, StrategicIP, Percep, Caution, Discount, Age, Edu, Income), \n                ~ (.x - mean(.x, na.rm = TRUE)) / (2 * sd(.x, na.rm = TRUE))),\n         across(c(Conservatism, Authoritarianism, Dogmatism), scale))\n\n\nd_des &lt;- describe(d)\nd_des %&gt;% \n  select(mean, sd)\n##                    mean   sd\n## ID               167.50 96.6\n## Age                0.00  0.5\n## Edu                0.00  0.5\n## Gender*            1.51  0.5\n## Income             0.00  0.5\n## SpeededIP          0.00  0.5\n## StrategicIP        0.00  0.5\n## Percep             0.00  0.5\n## Caution            0.00  0.5\n## Discount           0.00  0.5\n## Conservatism       0.00  1.0\n## Authoritarianism   0.00  1.0\n## Dogmatism          0.00  1.0\n\nAs I am curious, I create a second dataframe in which I use the more common approach to scale continuous predictor and outcome variables to have a mean of 0 and a sd of 1. To make effects comparable to a binary predictor, we use effect coding for the variable gender (see https://statmodeling.stat.columbia.edu/2009/06/09/standardization/)\n\n\nd1sd &lt;- d %&gt;% \n  mutate(across(where(is.numeric), scale))\n\ncontrasts(d1sd$Gender) &lt;- contr.sum(levels(d1sd$Gender))\n\nd1sd_des &lt;- describe(d1sd) \nd1sd_des %&gt;% \n  select(mean, sd)\n##                  mean  sd\n## ID               0.00 1.0\n## Age              0.00 1.0\n## Edu              0.00 1.0\n## Gender*          1.51 0.5\n## Income           0.00 1.0\n## SpeededIP        0.00 1.0\n## StrategicIP      0.00 1.0\n## Percep           0.00 1.0\n## Caution          0.00 1.0\n## Discount         0.00 1.0\n## Conservatism     0.00 1.0\n## Authoritarianism 0.00 1.0\n## Dogmatism        0.00 1.0\n\nI am very curious. Technically, not only gender, but also education is a categorical variable, and I want to treat it as such. Let’s look at how many numbers of observation we have for each level of education.\nSome factor levels have very few observations that do not all for comparisons. So let’s collapse them into a few, less levels.\n\ndf &lt;- dat %&gt;%\n  mutate(EduFactor = case_when(\n    Edu %in% c(1, 2) ~ \"High school (and lower)\",\n    Edu %in% c(3, 4) ~ \"Below Bachelor\",\n    Edu %in% c(5, 6, 7) ~ \"Bachelor, Master, Doctorate\",\n    TRUE ~ NA_character_  # This line is optional, handles unexpected cases\n  ))  %&gt;%\n  mutate(EduFactor = factor(EduFactor, levels = c(\"High school (and lower)\", \"Below Bachelor\", \"Bachelor, Master, Doctorate\")),\n          Gender = as.factor(Gender)) %&gt;% \n  mutate(across(where(is.numeric), scale))\n\nTo make the effects of different input variables comparable, let’s standardize numerical variables and use effect coding for categorical variables again.\n\n\n# Gender\ncontrasts(df$Gender) &lt;- contr.sum(levels(df$Gender))\ncontrasts(df$Gender)\n##   [,1]\n## 0    1\n## 1   -1\n\n# Education\n# Define the contrast matrix\ncontrast_edu &lt;- matrix(c(1, -1, -1,\n                        -1,  1, -1,\n                        -1, -1, -1),\n                      byrow = TRUE,  \n                      ncol = 3)      \ncontrasts(df$EduFactor) &lt;- contrast_edu\ncontrasts(df$EduFactor)\n##                             [,1] [,2]\n## High school (and lower)        1   -1\n## Below Bachelor                -1    1\n## Bachelor, Master, Doctorate   -1   -1\n\nThis coding ensures that each level’s effects are considered relative to the overall mean (similar to traditional effect coding), but with adjustments to balance the variance attributed to each level in the prior predictive distribution, by replacing 0s with -1s. I am not an expert on this so please see this for some literature on why this should work:https://discourse.mc-stan.org/t/symmetric-weakly-informative-priors-for-categorical-predictors/22188/5\nCreate yet another dataframe in which I use usual dummy-coding\n\ndf_dummy &lt;- dat %&gt;%\n  mutate(EduFactor = case_when(\n    Edu %in% c(1, 2) ~ \"High school (and lower)\",\n    Edu %in% c(3, 4) ~ \"Below Bachelor\",\n    Edu %in% c(5, 6, 7) ~ \"Bachelor, Master, Doctorate\",\n    TRUE ~ NA_character_  # This line is optional, handles unexpected cases\n  ))  %&gt;%\n  mutate(EduFactor = factor(EduFactor, levels = c(\"High school (and lower)\", \"Below Bachelor\", \"Bachelor, Master, Doctorate\")),\n          Gender = as.factor(Gender)) %&gt;% \n  mutate(across(where(is.numeric), scale))\n\nLet’s plot the distributions of our outcome variables next.\n\n\nvars1_density&lt;- df %&gt;%\n  select(c(Conservatism, Authoritarianism, Dogmatism)) %&gt;%\n  pivot_longer(., cols = c(Conservatism, Authoritarianism, Dogmatism), \n               names_to = \"Variable\", values_to = \"Value\") %&gt;%\n  ggplot(aes(x = Value, fill = factor(Variable))) + \n  geom_density(aes(alpha = 0.6)) + \n  labs(x=\"Standardised values (mean = 0, sd = 1)\", y=element_blank(),\n       title=\"Distributions of outcome variables\") +\n  scale_fill_ipsum() +\n  scale_color_ipsum() +\n  guides(color = \"none\", alpha = \"none\") +\n  theme_ipsum(plot_title_size = 14) + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\nvars1_density\n\n\n\n\n\n\n\nLet’s also plot the distributions of the predictor variables.\n\n\nvars1_density&lt;- df %&gt;%\n  select(SpeededIP, StrategicIP, Percep, Caution, Discount) %&gt;%\n  pivot_longer(., cols = c(SpeededIP, StrategicIP, Percep, Caution, Discount), \n               names_to = \"Variable\", values_to = \"Value\") %&gt;%\n  ggplot(aes(x = Value, fill = factor(Variable))) + \n  geom_density(aes(alpha = 0.4)) + \n  labs(x=\"Standardised values (mean = 0, sd = 1)\", y=element_blank(),\n       title=\"Distributions of predictor variables\") +\n  scale_fill_ipsum() +\n  scale_color_ipsum() +\n  guides(color = \"none\", alpha = \"none\") +\n  theme_ipsum(plot_title_size = 14) + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\nvars1_density\n\n\n\n\n\n\n\nHave a first visual look of the association between the variables.\n\n\n# predictor and outcome variables\npredictors &lt;- c(\"SpeededIP\", \"StrategicIP\", \"Percep\", \"Caution\", \"Discount\")\noutcomes &lt;- c(\"Conservatism\", \"Authoritarianism\", \"Dogmatism\")\n\n# list to store sub-lists of plots (one list per outcome variable)\nplots_lists &lt;- vector(\"list\", length(outcomes))\nnames(plots_lists) &lt;- outcomes\n\nfor (outcome in outcomes) {\n  # Initialize an empty list to store plots for this particular outcome\n  plots &lt;- list()\n  \n  for (predictor in predictors) {\n    # Generate the plot for this predictor-outcome pair\n    plot &lt;- ggplot(df, aes_string(x = predictor, y = outcome)) +\n      geom_point(size = 1) +\n      geom_smooth(method = \"lm\") +\n      labs(x = predictor, y = outcome,\n           title = paste(predictor, \"and\", outcome)) +\n      scale_fill_ipsum() +\n      scale_color_ipsum() +\n      guides(color = \"none\", alpha = \"none\") +\n      theme_ipsum(plot_title_size = 12)\n      \n    # Add the plot to the list of plots\n    plots[[predictor]] &lt;- plot\n  }\n  \n  # Combine all plots for this outcome into a single patchwork object\n  plots_lists[[outcome]] &lt;- reduce(plots, `+`)\n}\n\n\n\nprint(plots_lists[[\"Conservatism\"]])\n\n\n\n\n\n\n\n\n\nprint(plots_lists[[\"Authoritarianism\"]])\n\n\n\n\n\n\n\n\n\nprint(plots_lists[[\"Dogmatism\"]])\n\n\n\n\n\n\n\nNow that we have a first descriptive impression of what the relationships may look like, let’s start with the first model.\nFrequentist multiple linear regression: The associations between cognitive factors and political conservatism\nThis time, I will also fit a linear regressions using the frequentist lm() function. We will use the results from this to make comparisons to the Bayesian approach. We start the following model:\nConservatism ~ SpeededIP + StrategicIP + Percep + Caution + Discount + Gender + Age + Edu + Income and will compare the coefficients for the dataset in which all continuous input variables were scaled by 2 SD with the one in which they were scaled by 2 SD.\n\n\nf_m1_con &lt;- lm(Conservatism ~ SpeededIP + StrategicIP + Percep + Caution + Discount + Gender + Age + Edu + Income, data = d)\n\nsummary(f_m1_con)\n## \n## Call:\n## lm(formula = Conservatism ~ SpeededIP + StrategicIP + Percep + \n##     Caution + Discount + Gender + Age + Edu + Income, data = d)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.4766 -0.6220  0.0195  0.6279  2.8572 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.10477    0.07402    1.42   0.1579    \n## SpeededIP    0.10288    0.12156    0.85   0.3980    \n## StrategicIP -0.48601    0.12043   -4.04  6.9e-05 ***\n## Percep       0.12898    0.10987    1.17   0.2413    \n## Caution      0.28971    0.10779    2.69   0.0076 ** \n## Discount     0.18185    0.11156    1.63   0.1041    \n## Gender1     -0.18396    0.10648   -1.73   0.0851 .  \n## Age          0.30968    0.11767    2.63   0.0089 ** \n## Edu          0.00537    0.11191    0.05   0.9618    \n## Income       0.08150    0.10856    0.75   0.4534    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.91 on 309 degrees of freedom\n##   (15 observations deleted due to missingness)\n## Multiple R-squared:  0.164,  Adjusted R-squared:  0.14 \n## F-statistic: 6.73 on 9 and 309 DF,  p-value: 8.14e-09\n\nWe can see that there is a statistically significant effect (with \\(\\alpha = 0.05\\)) for StrategicIP and Caution, as well as Age.\nWe will not go into detail for now, but let’s also fit the same regression on the data that was standardized to have a SD = 1.\n\n\nf_m2_con &lt;- lm(Conservatism ~ SpeededIP + StrategicIP + Percep + Caution + Discount + Gender + Age + Edu + Income, data = d1sd)\n\nsummary(f_m2_con)\n## \n## Call:\n## lm(formula = Conservatism ~ SpeededIP + StrategicIP + Percep + \n##     Caution + Discount + Gender + Age + Edu + Income, data = d1sd)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.4766 -0.6220  0.0195  0.6279  2.8572 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.01279    0.05104    0.25   0.8022    \n## SpeededIP    0.05144    0.06078    0.85   0.3980    \n## StrategicIP -0.24300    0.06021   -4.04  6.9e-05 ***\n## Percep       0.06449    0.05494    1.17   0.2413    \n## Caution      0.14485    0.05390    2.69   0.0076 ** \n## Discount     0.09092    0.05578    1.63   0.1041    \n## Gender1      0.09198    0.05324    1.73   0.0851 .  \n## Age          0.15484    0.05884    2.63   0.0089 ** \n## Edu          0.00268    0.05596    0.05   0.9618    \n## Income       0.04075    0.05428    0.75   0.4534    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.91 on 309 degrees of freedom\n##   (15 observations deleted due to missingness)\n## Multiple R-squared:  0.164,  Adjusted R-squared:  0.14 \n## F-statistic: 6.73 on 9 and 309 DF,  p-value: 8.14e-09\n\nAlright, makes sense. If we scale the input variables by 2 SD, the estimates are exactly two times as large, a change of 2 SD of the predictor reflect a change of X SD in the outcome.\nIf we scale the input by one SD, a change of 1 SD of the predictor variables reflect a change of X SD in the outcome.\nLet’s check what happens when we scale the input variables by 1 SD, but set up sum-coding for the categorical variables.\n\n\nf_m3_con &lt;- lm(Conservatism ~ SpeededIP + StrategicIP + Percep + Caution + Discount + Gender + Age + EduFactor + Income, data = df)\n\nsummary(f_m3_con)\n## \n## Call:\n## lm(formula = Conservatism ~ SpeededIP + StrategicIP + Percep + \n##     Caution + Discount + Gender + Age + EduFactor + Income, data = df)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.4816 -0.6105  0.0379  0.6197  2.8478 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -0.0125     0.0788   -0.16   0.8744    \n## SpeededIP     0.0512     0.0609    0.84   0.4006    \n## StrategicIP  -0.2465     0.0605   -4.08  5.8e-05 ***\n## Percep        0.0638     0.0550    1.16   0.2467    \n## Caution       0.1408     0.0542    2.60   0.0099 ** \n## Discount      0.0959     0.0559    1.72   0.0871 .  \n## Gender1       0.0877     0.0537    1.63   0.1037    \n## Age           0.1561     0.0589    2.65   0.0085 ** \n## EduFactor1   -0.0244     0.0775   -0.31   0.7534    \n## EduFactor2   -0.0334     0.0605   -0.55   0.5810    \n## Income        0.0356     0.0547    0.65   0.5165    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.911 on 308 degrees of freedom\n##   (15 observations deleted due to missingness)\n## Multiple R-squared:  0.165,  Adjusted R-squared:  0.138 \n## F-statistic: 6.07 on 10 and 308 DF,  p-value: 1.93e-08\n\nWe achieved the same thing! But its actually a bit easier to interpret effects now. So I think I actually prefer this way and will stick with it for now. We scale our input variables by 1 SD, and use sum-coding for our binary predictor variables.\nNow that we cleared this - let’s finally get to the point of this blog post.\nBayesian multiple linear regression\nAlright, let’s do the same but in the Bayesian framework. For now, we will stick again and investigate the associations of the cognitive factors with political conservatism while controlling for the effects of the confounding variables.\nSelecting priors\nI already scaled all variables of interest. This will make it a bit simpler to select priors. I will use weakly informative priors for our slope and intercept parameters (see https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations). The selected prior is \\(N(0,1)\\). Let’s plot to prior to see what kind of an effect it may have on our parameters.\n\n\ncolors &lt;- ipsum_pal()(1)\n\nprior(normal(0, 1)) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(.width = 0.683, p_limits = c(.001, .999), \n               slab_fill = colors[1], slab_alpha = 0.8) +\n  scale_y_discrete(NULL, breaks = NULL, expand = expansion(add = 0.1)) +\n  labs(title = \"Prior N(0,1)\", x = \"Parameter Space\", y = \"\") +\n  coord_cartesian(xlim = c(-4, 4)) +\n  scale_x_continuous(breaks = seq(-4, 4, by = 1)) +\n  theme_ipsum() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()) \n\n\n\n\n\n\n\nWe can see that in the the plot for \\(N(0,1)\\), more density is around 0. As we expect our parameter values to be between -1 and 1, this gives more weight to parameter values closer to zero, and less weight to parameter values that are far from zero.\nLet’s check how our priors are set by default in brms.\n\nget_prior(Conservatism ~ SpeededIP + StrategicIP + Percep + Caution + Discount + Gender + Age + EduFactor + Income, data = df)\n## Warning: Rows containing NAs were excluded from the model.\n##                 prior     class        coef group resp dpar nlpar lb ub       source\n##                (flat)         b                                              default\n##                (flat)         b         Age                             (vectorized)\n##                (flat)         b     Caution                             (vectorized)\n##                (flat)         b    Discount                             (vectorized)\n##                (flat)         b  EduFactor1                             (vectorized)\n##                (flat)         b  EduFactor2                             (vectorized)\n##                (flat)         b     Gender1                             (vectorized)\n##                (flat)         b      Income                             (vectorized)\n##                (flat)         b      Percep                             (vectorized)\n##                (flat)         b   SpeededIP                             (vectorized)\n##                (flat)         b StrategicIP                             (vectorized)\n##  student_t(3, 0, 2.5) Intercept                                              default\n##  student_t(3, 0, 2.5)     sigma                                    0         default\n\nBy default brms sets non-informative flat priors for our slope parameters. We thus have to change this, and can do so directly when fitting the model.\nFitting and summarising the first model\n\nm1.1 &lt;- brm(Conservatism ~ SpeededIP + StrategicIP + Percep + Caution + Discount + Gender + Age + EduFactor + Income, \n          data = df, \n          prior = c(prior(normal(0, 1), class = \"Intercept\"), \n                    prior(normal(0, 1), class = \"b\")),\n          seed = bayes_seed,\n          file = \"02_fits/fit_m1.1\")\n\nWe get a warning message:\nWarning: Rows containing NAs were excluded from the model.\nWe are not happy with this, but for now we will deal with it later.\nCheck the results. We can use either summary(m1.1) or print(m1.1) to do so.\n\nsummary(m1.1, prob = 0.95)  # prob = 0.95 is the default\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: Conservatism ~ SpeededIP + StrategicIP + Percep + Caution + Discount + Gender + Age + EduFactor + Income \n##    Data: df (Number of observations: 319) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept      -0.01      0.08    -0.18     0.15 1.00     6802     3047\n## SpeededIP       0.05      0.06    -0.07     0.17 1.00     5675     3118\n## StrategicIP    -0.25      0.06    -0.36    -0.13 1.00     5877     3118\n## Percep          0.06      0.06    -0.05     0.17 1.00     7396     2862\n## Caution         0.14      0.05     0.04     0.25 1.00     7209     3474\n## Discount        0.10      0.06    -0.01     0.21 1.00     6027     3021\n## Gender1         0.09      0.05    -0.02     0.19 1.00     7069     3145\n## Age             0.16      0.06     0.03     0.27 1.00     5988     3108\n## EduFactor1     -0.03      0.08    -0.18     0.12 1.00     6084     3451\n## EduFactor2     -0.03      0.06    -0.15     0.09 1.00     6409     3374\n## Income          0.04      0.06    -0.07     0.14 1.00     6764     3353\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.92      0.04     0.85     0.99 1.00     8581     3207\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\nUse the tools from bayestestR to have another look at the posterior. We also define a region of practical equivalence ranging from [-0.05; 0.05].\nMore on this later.\n\nm1.1_posteriors &lt;- describe_posterior(m1.1, \n                                      centrality = \"median\",\n                                      dispersion = FALSE,\n                                      ci = 0.95,\n                                      ci_method = \"hdi\",\n                                      test = c(\"rope\"),\n                                      rope_range = c(-0.05, 0.05),\n                                      rope_ci = 1)\n\nm1.1_posteriors &lt;- as.data.frame(m1.1_posteriors)\nm1.1_posteriors %&gt;% \n  select(Parameter, Median, CI_low, CI_high, ROPE_Percentage) %&gt;% \n  tt(.)\n\n \n\n  \n    \n\ntinytable_l7z3lhr43xynmp5512k1\n\n\n      \n\nParameter\n                Median\n                CI_low\n                CI_high\n                ROPE_Percentage\n              \n\n\nb_Intercept\n                  -0.0143\n                  -0.1656\n                   0.1552\n                  0.4522\n                \n\nb_SpeededIP\n                   0.0527\n                  -0.0732\n                   0.1666\n                  0.4313\n                \n\nb_StrategicIP\n                  -0.2467\n                  -0.3595\n                  -0.1274\n                  0.0005\n                \n\nb_Percep\n                   0.0634\n                  -0.0483\n                   0.1683\n                  0.3787\n                \n\nb_Caution\n                   0.1406\n                   0.0337\n                   0.2435\n                  0.0447\n                \n\nb_Discount\n                   0.0951\n                  -0.0185\n                   0.1993\n                  0.1970\n                \n\nb_Gender1\n                   0.0876\n                  -0.0262\n                   0.1872\n                  0.2405\n                \n\nb_Age\n                   0.1562\n                   0.0383\n                   0.2743\n                  0.0410\n                \n\nb_EduFactor1\n                  -0.0254\n                  -0.1785\n                   0.1241\n                  0.4492\n                \n\nb_EduFactor2\n                  -0.0338\n                  -0.1503\n                   0.0874\n                  0.5148\n                \n\nb_Income\n                   0.0359\n                  -0.0713\n                   0.1443\n                  0.5463\n                \n\n\n\n\n    \n\n\nPlot the conditional effects - for now just plot those of strategic information processing, this predictor seems to be largest in size. Let’s borrow some code (as often) from great open resources.\nFirst load some fancy colors from the beyonce package.\n\n# devtools::install_github(\"dill/beyonce\")\nlibrary(beyonce)\n\nbp &lt;- beyonce_palette(41, n = 9, type = \"continuous\")\nbp\n\n\n\n\n\n\n\nLet’s gather the draws from our model as a dataframe.\n\nw_draws &lt;- as_draws_df(m1.1)\nhead(w_draws)\n## # A draws_df: 6 iterations, 1 chains, and 14 variables\n##   b_Intercept b_SpeededIP b_StrategicIP b_Percep b_Caution b_Discount b_Gender1 b_Age\n## 1       0.125      -0.125         -0.22   0.0597      0.21     -0.013     0.058 0.127\n## 2      -0.070       0.193         -0.24   0.0461      0.14      0.182     0.164 0.210\n## 3      -0.067       0.156         -0.28   0.0358      0.14      0.186     0.137 0.164\n## 4       0.050       0.055         -0.27   0.1670      0.15      0.068     0.149 0.055\n## 5      -0.063       0.033         -0.31  -0.0078      0.15      0.113     0.044 0.242\n## 6      -0.085       0.109         -0.32   0.0486      0.19      0.070     0.065 0.204\n## # ... with 6 more variables\n## # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\nLet’s first replicate the plot from above.\n\n\n# how many posterior lines would you like?\nn_lines &lt;- 200\n\ndf %&gt;% \n  ggplot(aes(x = StrategicIP, y = Conservatism)) +\n  geom_abline(data = w_draws %&gt;% slice(1:n_lines),\n              aes(intercept = b_Intercept, slope = b_StrategicIP, group = .draw),\n              color = bp[2], linewidth = 1/4, alpha = 1/4) + \n  geom_point(alpha = 1/2, color = bp[5]) +\n  labs(title = \"Conditional association between StrategicIP and Conservatism\",\n       caption = (paste(\"Data with\", n_lines, \"credible regression lines\")),\n       x = \"Strategic\",\n       y = \"Conservatism\") +\n  coord_cartesian(xlim = c(-5, 5),\n                  ylim = c(-3, 3)) +\n  theme_ipsum(plot_title_size = 12,\n              axis_title_size = 10,\n              axis_title_face = \"bold\") +\n  theme(panel.grid.minor = element_blank()) \n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not\n## found in Windows font database\n## Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not found in\n## Windows font database\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not\n## found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not\n## found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not\n## found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not\n## found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family\n## not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family\n## not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family\n## not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family\n## not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family\n## not found in Windows font database\n\n\n\n\n\n\n\nLet’s also plot the conditional association between Age and Conservatism. As we standardized the variable Age, but Age in years is more intuitive for interpretations, let’s rescale the draws first.\n\n\n# make a function to rescale slope estimates\nmake_beta_1 &lt;- function(zeta_1, sd_x, sd_y) {\n  zeta_1 * sd_y / sd_x\n}\n\n# this is not super elegant but I add Conservatism scaled to the first data as in df I only have the scaled variable - note for future do not change orig variable\ndata &lt;- data %&gt;% \n  mutate(Conservatism_z = scale(Conservatism))\n\n# calculate the sd of x and y\nsd_x &lt;- sd(data$Age, na.rm = T)\nsd_y &lt;- sd(data$Conservatism_z, na.rm = T)\n\n# rescale the posterior draws of Age\nw_draws &lt;-\n  w_draws %&gt;% \n  mutate(b_Age_orig = make_beta_1(zeta_1 = b_Age,\n                                  sd_x   = sd_x,\n                                  sd_y   = sd_y))\n\nw_draws %&gt;% \n  select(b_Age, b_Age_orig) %&gt;% \n  head(.) %&gt;% \n  tt(.)\n## Warning: Dropping 'draws_df' class as required metadata was removed.\n\n \n\n  \n    \n\ntinytable_fpwsovgd5fuxtv5lu5je\n\n\n      \n\nb_Age\n                b_Age_orig\n              \n\n\n0.126837715268519\n                  0.0149471688486139\n                \n\n0.210043509129804\n                  0.0247525413862275\n                \n\n0.16379642037851\n                  0.0193025611271303\n                \n\n0.0548954843510674\n                  0.00646914895845259\n                \n\n0.241954146956206\n                  0.0285130450396435\n                \n\n0.203877101602445\n                  0.0240258621464944\n                \n\n\n\n\n    \n\n\nNow we are ready to create a prettier plot from our manual draws for Age.\n\n\n# how many posterior lines would you like?\nn_lines &lt;- 200\n\ndata %&gt;% \n  ggplot(aes(x = Age, y = Conservatism_z)) +\n  geom_abline(data = w_draws %&gt;% slice(1:n_lines),\n              aes(intercept = b_Intercept, slope = b_Age_orig, group = .draw),\n              color = bp[2], linewidth = 1/4, alpha = 1/4) + \n  geom_point(alpha = 1/2, color = bp[5]) +\n  labs(title = \"Conditional association between Age and Conservatism\",\n       caption = (paste(\"Data with\", n_lines, \"credible regression lines\")),\n       x = \"Age\",\n       y = \"Conservatism\") +\n  coord_cartesian(xlim = c(18, 65),\n                  ylim = c(-3, 3)) +\n  theme_ipsum(plot_title_size = 12,\n              axis_title_size = 10,\n              axis_title_face = \"bold\") +\n  theme(panel.grid.minor = element_blank()) \n## Warning: Removed 9 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not\n## found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not\n## found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not\n## found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not\n## found in Windows font database\n\n## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family not\n## found in Windows font database\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family\n## not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family\n## not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family\n## not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family\n## not found in Windows font database\n\n## Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, : font family\n## not found in Windows font database\n\n\n\n\n\n\n\nLet’s also plot the conditional effects for our factor variables and see how those plots look like. For this we use the inbuilt function conditional_effects().\n\n\nc_eff_m1.1 &lt;- conditional_effects(m1.1)\n\nc_eff_m1.1_plot_gen &lt;- plot(c_eff_m1.1, \n                        points = T,\n                        point_args = list(size = 1, alpha = 1/4, width = .05, height = .05, color = \"black\"),\n                        plot = FALSE)[[6]] +\n  scale_color_ipsum() +\n  scale_fill_ipsum() +\n  theme_ipsum()\n\n\nc_eff_m1.1_plot_edu &lt;- plot(c_eff_m1.1, \n                        points = T,\n                        point_args = list(size = 1, alpha = 1/4, width = .05, height = .05, color = \"black\"),\n                        plot = FALSE)[[8]] +\n  scale_color_ipsum() +\n  scale_fill_ipsum() +\n  theme_ipsum()\n  \nc_eff_m1.1_plot_gen + c_eff_m1.1_plot_edu\n\n\n\n\n\n\n\nLet’s use bayesplot functions to plot the posterior distributions of the parameters.\n\n\nmcmc_plot(m1.1, \n          type = \"areas\") +\n  theme_ipsum()\n\n\n\n\n\n\n\nWe can also manually create a similar plot using draws from the posterior distribution. For simplicity, Here we use gather_draws from the tidybayes package - this automatically returns a long dataframe suitable for plotting in ggplot.\n\n\n# gather draws in a long format\ndraws_m1.1 &lt;- m1.1 %&gt;% \n  gather_draws(`b_.*`, regex = TRUE)  %&gt;% \n  filter(.variable %in% c(\"b_SpeededIP\", \"b_StrategicIP\", \"b_Percep\", \"b_Caution\", \"b_Discount\")) %&gt;%\n  mutate(.variable = case_when(\n    .variable == \"b_SpeededIP\" ~ \"Speed of Evidence Accumulation\",\n    .variable == \"b_StrategicIP\" ~ \"Strategic Information Processing\",\n    .variable == \"b_Percep\" ~ \"Perceptual Processing Time\",\n    .variable == \"b_Caution\" ~ \"Caution\",\n    .variable == \"b_Discount\" ~ \"Discount\",\n    TRUE ~ .variable # keeps the original value if none of the conditions are met\n  ))  %&gt;%\n  mutate(.variable = factor(.variable, levels = c(\n    \"Speed of Evidence Accumulation\",\n    \"Strategic Information Processing\",\n    \"Perceptual Processing Time\",\n    \"Caution\",\n    \"Discount\"\n  )))\n\nggplot(draws_m1.1, aes(x = .value, y = fct_rev(.variable), fill = .variable)) +\n  geom_rect(aes(xmin = -0.05, xmax = 0.05, ymin = -Inf, ymax = Inf), \n            color = \"transparent\", fill = bp[9], alpha = 0.05) +\n  geom_vline(xintercept = 0, alpha = 0.2) +\n  stat_halfeye(slab_alpha = 0.8,\n               .width = c(0.9, 0.95), point_interval = \"median_hdi\") +\n  scale_fill_manual(values = beyonce_palette(41, n = 7, type = \"continuous\")) +\n  guides(fill = \"none\", slab_alpha = \"none\", alpha = \"none\") +\n  labs(title = \"Associations of Cognitive Factors with Political Conservatism\", \n       x = \"Posterior Distribution\", \n       y = \"Variable\",\n       caption = \"90% and 95% credible intervals shown in black.\") +\n  scale_x_continuous(breaks = seq(-0.50, 0.50, by = 0.1)) +\n  coord_cartesian(xlim = c(-0.45, 0.45)) +\n  theme_ipsum(plot_title_size = 14,\n              axis_title_size = 12,\n              axis_title_face = \"bold\") +\n  annotate(\"text\", x = 0, y = 0.6, label = \"ROPE\", \n            color = \"black\", size = 5)\n\n\n\n\n\n\n\nInterpretation of the findings\nLet’s make some sense of the findings.\nThe model indicates that there is a high probability given the data that Strategic Information Processing (Median = -0.25 , 95% CI [-0.36; -0.13]) is negatively associated with Conservatism, and that in turn, Caution (Median = 0.14 , 95% CI [0.03; 0.24]) is positively associated with Conservatism.\nChecking the model\nPrior check\nLet’s check if the prior that we used for the slope and intercept was informative or, as intended, only weakly informative. We can use functionality from bayestestR for this.\n\n\nprior_summary(m1.1)\n##                 prior     class        coef group resp dpar nlpar lb ub       source\n##          normal(0, 1)         b                                                 user\n##          normal(0, 1)         b         Age                             (vectorized)\n##          normal(0, 1)         b     Caution                             (vectorized)\n##          normal(0, 1)         b    Discount                             (vectorized)\n##          normal(0, 1)         b  EduFactor1                             (vectorized)\n##          normal(0, 1)         b  EduFactor2                             (vectorized)\n##          normal(0, 1)         b     Gender1                             (vectorized)\n##          normal(0, 1)         b      Income                             (vectorized)\n##          normal(0, 1)         b      Percep                             (vectorized)\n##          normal(0, 1)         b   SpeededIP                             (vectorized)\n##          normal(0, 1)         b StrategicIP                             (vectorized)\n##          normal(0, 1) Intercept                                                 user\n##  student_t(3, 0, 2.5)     sigma                                    0         default\ncheck_prior(m1.1)\n##        Parameter Prior_Quality\n## 1    b_Intercept uninformative\n## 2    b_SpeededIP uninformative\n## 3  b_StrategicIP uninformative\n## 4       b_Percep uninformative\n## 5      b_Caution uninformative\n## 6     b_Discount uninformative\n## 7      b_Gender1 uninformative\n## 8          b_Age uninformative\n## 9   b_EduFactor1 uninformative\n## 10  b_EduFactor2 uninformative\n## 11      b_Income uninformative\n\nOk - as intended our priors were uninformative.\nPosterior predictive check\nLet’s also check how well our model performs in generating data that is similar to our observed data. We call this a Posterior predictive check (PPC).\n\npp_check(m1.1, ndraws = 100)\n\n\n\n\n\n\n\nOur model seems to capture the shape of the observed data well.\n\npp_check(m1.1, ndraws = 100, type ='error_scatter_avg', alpha = .1)\n\n\n\n\n\n\n\nTrace plot\nCommonly, we look at the trace plots, i.e., the estimated values across each iteration for a chain. See this nice practical guide by Michael Clark for some details. We will follow his code in several steps below.\nWhat should the trace plot look like? Ideally it should look like a series from a normal distribution.\n\nqplot(x = 1:1000, y = rnorm(1000), geom = 'line') +\n  theme_ipsum()\n\n\n\n\n\n\n\nNow we can also plot the trace plot from our model, using nice functionality from bayesplot.\n\nmcmc_plot(m1.1, type = 'trace') +\n  theme_ipsum()\n\n\n\n\n\n\n\nThis looks good too.\nTime to go Multivariate\nOk - but what if we want to investigate the effects of multiple predictors on multiple outcomes at the same time? brms allows us to run a multivariate multiple regression! So we can do just that.\nWe will keep it a bit brief here, but may delve into this a bit more in future blog posts.\nStart with formulating our model. We want to investigate the relative controlled associations between the five Cognitive Factors and Conservatism, Authoritarianism and Dogmatism at the same time.\n\nf_mv1 &lt;- \n  bf(mvbind(Conservatism, Authoritarianism, Dogmatism) \n     ~ SpeededIP + StrategicIP + Percep + Caution + Discount + \n       Gender + Age + EduFactor + Income) +\n  set_rescor(TRUE)\n\nLet’s have a look at the default priors.\n\nget_prior(data = df,\n          family = gaussian,\n          formula = f_mv1)\n## Warning: Rows containing NAs were excluded from the model.\n##                    prior     class        coef group             resp dpar nlpar lb ub\n##                   (flat)         b                                                    \n##                   (flat) Intercept                                                    \n##                   lkj(1)    rescor                                                    \n##                   (flat)         b                   Authoritarianism                 \n##                   (flat)         b         Age       Authoritarianism                 \n##                   (flat)         b     Caution       Authoritarianism                 \n##                   (flat)         b    Discount       Authoritarianism                 \n##                   (flat)         b  EduFactor1       Authoritarianism                 \n##                   (flat)         b  EduFactor2       Authoritarianism                 \n##                   (flat)         b     Gender1       Authoritarianism                 \n##                   (flat)         b      Income       Authoritarianism                 \n##                   (flat)         b      Percep       Authoritarianism                 \n##                   (flat)         b   SpeededIP       Authoritarianism                 \n##                   (flat)         b StrategicIP       Authoritarianism                 \n##  student_t(3, -0.1, 2.5) Intercept                   Authoritarianism                 \n##     student_t(3, 0, 2.5)     sigma                   Authoritarianism             0   \n##                   (flat)         b                       Conservatism                 \n##                   (flat)         b         Age           Conservatism                 \n##                   (flat)         b     Caution           Conservatism                 \n##                   (flat)         b    Discount           Conservatism                 \n##                   (flat)         b  EduFactor1           Conservatism                 \n##                   (flat)         b  EduFactor2           Conservatism                 \n##                   (flat)         b     Gender1           Conservatism                 \n##                   (flat)         b      Income           Conservatism                 \n##                   (flat)         b      Percep           Conservatism                 \n##                   (flat)         b   SpeededIP           Conservatism                 \n##                   (flat)         b StrategicIP           Conservatism                 \n##     student_t(3, 0, 2.5) Intercept                       Conservatism                 \n##     student_t(3, 0, 2.5)     sigma                       Conservatism             0   \n##                   (flat)         b                          Dogmatism                 \n##                   (flat)         b         Age              Dogmatism                 \n##                   (flat)         b     Caution              Dogmatism                 \n##                   (flat)         b    Discount              Dogmatism                 \n##                   (flat)         b  EduFactor1              Dogmatism                 \n##                   (flat)         b  EduFactor2              Dogmatism                 \n##                   (flat)         b     Gender1              Dogmatism                 \n##                   (flat)         b      Income              Dogmatism                 \n##                   (flat)         b      Percep              Dogmatism                 \n##                   (flat)         b   SpeededIP              Dogmatism                 \n##                   (flat)         b StrategicIP              Dogmatism                 \n##     student_t(3, 0, 2.5) Intercept                          Dogmatism                 \n##     student_t(3, 0, 2.5)     sigma                          Dogmatism             0   \n##        source\n##       default\n##       default\n##       default\n##       default\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##       default\n##       default\n##       default\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##       default\n##       default\n##       default\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##  (vectorized)\n##       default\n##       default\n\nAgain, we want to set the priors for the intercepts and slopes to \\(N(0, 1)\\).\nLet’s fit the model.\n\nmv1 &lt;- brm(data = df,\n           family = gaussian,\n           formula = f_mv1,\n           prior = c(prior(normal(0, 1), class = \"Intercept\"),\n                     prior(normal(0, 1), class = \"b\")),\n          seed = bayes_seed,\n          file = \"02_fits/fit_mv1\")\n\nLet’s have a look at the output.\n\nsummary(mv1)\n##  Family: MV(gaussian, gaussian, gaussian) \n##   Links: mu = identity; sigma = identity\n##          mu = identity; sigma = identity\n##          mu = identity; sigma = identity \n## Formula: Conservatism ~ SpeededIP + StrategicIP + Percep + Caution + Discount + Gender + Age + EduFactor + Income \n##          Authoritarianism ~ SpeededIP + StrategicIP + Percep + Caution + Discount + Gender + Age + EduFactor + Income \n##          Dogmatism ~ SpeededIP + StrategicIP + Percep + Caution + Discount + Gender + Age + EduFactor + Income \n##    Data: df (Number of observations: 319) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##                              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Conservatism_Intercept          -0.01      0.08    -0.17     0.14 1.00     3468     2921\n## Authoritarianism_Intercept       0.02      0.08    -0.15     0.18 1.00     3954     3213\n## Dogmatism_Intercept              0.04      0.08    -0.12     0.21 1.00     4130     3532\n## Conservatism_SpeededIP           0.05      0.06    -0.07     0.17 1.00     4090     3362\n## Conservatism_StrategicIP        -0.24      0.06    -0.37    -0.12 1.00     3945     2937\n## Conservatism_Percep              0.06      0.05    -0.05     0.17 1.00     4061     2961\n## Conservatism_Caution             0.14      0.06     0.03     0.25 1.00     4148     3692\n## Conservatism_Discount            0.10      0.06    -0.02     0.21 1.00     4386     3381\n## Conservatism_Gender1             0.09      0.05    -0.02     0.19 1.00     4612     3568\n## Conservatism_Age                 0.16      0.06     0.04     0.27 1.00     3767     3305\n## Conservatism_EduFactor1         -0.03      0.08    -0.18     0.13 1.00     3561     3135\n## Conservatism_EduFactor2         -0.03      0.06    -0.15     0.09 1.00     3489     2839\n## Conservatism_Income              0.04      0.06    -0.07     0.14 1.00     4206     3325\n## Authoritarianism_SpeededIP       0.10      0.07    -0.03     0.23 1.00     3671     3293\n## Authoritarianism_StrategicIP    -0.11      0.07    -0.24     0.02 1.00     3674     3107\n## Authoritarianism_Percep          0.01      0.06    -0.10     0.13 1.00     4530     3466\n## Authoritarianism_Caution         0.07      0.06    -0.05     0.19 1.00     4071     3241\n## Authoritarianism_Discount        0.13      0.06     0.01     0.25 1.00     4400     3208\n## Authoritarianism_Gender1        -0.09      0.06    -0.20     0.03 1.00     4212     3202\n## Authoritarianism_Age             0.07      0.06    -0.05     0.19 1.00     3676     3390\n## Authoritarianism_EduFactor1      0.05      0.08    -0.12     0.21 1.00     3715     3523\n## Authoritarianism_EduFactor2     -0.06      0.06    -0.18     0.07 1.00     4004     3529\n## Authoritarianism_Income          0.04      0.06    -0.08     0.16 1.00     3776     3050\n## Dogmatism_SpeededIP             -0.10      0.07    -0.23     0.02 1.00     4151     3242\n## Dogmatism_StrategicIP           -0.09      0.07    -0.22     0.03 1.00     4011     3179\n## Dogmatism_Percep                 0.05      0.06    -0.07     0.16 1.00     4177     3449\n## Dogmatism_Caution                0.06      0.06    -0.06     0.18 1.00     4558     3399\n## Dogmatism_Discount               0.06      0.06    -0.06     0.17 1.00     4534     3168\n## Dogmatism_Gender1               -0.01      0.06    -0.13     0.10 1.00     4428     3170\n## Dogmatism_Age                   -0.01      0.06    -0.14     0.11 1.00     4063     3184\n## Dogmatism_EduFactor1             0.07      0.08    -0.09     0.23 1.00     4118     3439\n## Dogmatism_EduFactor2            -0.03      0.07    -0.16     0.09 1.00     3827     3292\n## Dogmatism_Income                -0.01      0.06    -0.13     0.10 1.00     4682     3298\n## \n## Family Specific Parameters: \n##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_Conservatism         0.92      0.04     0.85     0.99 1.00     4289     3204\n## sigma_Authoritarianism     0.98      0.04     0.91     1.06 1.00     4699     3296\n## sigma_Dogmatism            0.99      0.04     0.92     1.07 1.00     4893     2919\n## \n## Residual Correlations: \n##                                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\n## rescor(Conservatism,Authoritarianism)     0.42      0.05     0.33     0.51 1.00     4369\n## rescor(Conservatism,Dogmatism)            0.32      0.05     0.22     0.42 1.00     4756\n## rescor(Authoritarianism,Dogmatism)        0.31      0.05     0.21     0.41 1.00     4094\n##                                       Tail_ESS\n## rescor(Conservatism,Authoritarianism)     2693\n## rescor(Conservatism,Dogmatism)            3229\n## rescor(Authoritarianism,Dogmatism)        2905\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nmv1_posteriors &lt;- describe_posterior(mv1, \n                                     centrality = \"median\",\n                                     dispersion = FALSE,\n                                     ci = 0.95,\n                                     ci_method = \"hdi\")\n## Warning: Multivariate response models are not yet supported for tests `rope` and\n## `p_rope`.\n\nmv1_posteriors &lt;- as.data.frame(mv1_posteriors)\nmv1_posteriors %&gt;% \n  select(Parameter, Median, CI_low, CI_high) %&gt;% \n  mutate(across(where(is.numeric), ~round(.x, 2))) %&gt;% \n  tt(.)\n\n \n\n  \n    \n\ntinytable_g6ujc0zkdzva1r2m37t5\n\n\n      \n\nParameter\n                Median\n                CI_low\n                CI_high\n              \n\n\nb_Conservatism_Intercept\n                  -0.01\n                  -0.16\n                   0.15\n                \n\nb_Conservatism_SpeededIP\n                   0.05\n                  -0.07\n                   0.17\n                \n\nb_Conservatism_StrategicIP\n                  -0.24\n                  -0.37\n                  -0.13\n                \n\nb_Conservatism_Percep\n                   0.06\n                  -0.05\n                   0.17\n                \n\nb_Conservatism_Caution\n                   0.14\n                   0.04\n                   0.25\n                \n\nb_Conservatism_Discount\n                   0.09\n                  -0.02\n                   0.20\n                \n\nb_Conservatism_Gender1\n                   0.09\n                  -0.02\n                   0.19\n                \n\nb_Conservatism_Age\n                   0.16\n                   0.04\n                   0.27\n                \n\nb_Conservatism_EduFactor1\n                  -0.02\n                  -0.18\n                   0.12\n                \n\nb_Conservatism_EduFactor2\n                  -0.04\n                  -0.15\n                   0.09\n                \n\nb_Conservatism_Income\n                   0.04\n                  -0.07\n                   0.14\n                \n\nb_Authoritarianism_Intercept\n                   0.02\n                  -0.15\n                   0.18\n                \n\nb_Authoritarianism_SpeededIP\n                   0.10\n                  -0.02\n                   0.23\n                \n\nb_Authoritarianism_StrategicIP\n                  -0.11\n                  -0.23\n                   0.02\n                \n\nb_Authoritarianism_Percep\n                   0.01\n                  -0.10\n                   0.13\n                \n\nb_Authoritarianism_Caution\n                   0.07\n                  -0.04\n                   0.19\n                \n\nb_Authoritarianism_Discount\n                   0.13\n                   0.01\n                   0.25\n                \n\nb_Authoritarianism_Gender1\n                  -0.09\n                  -0.19\n                   0.03\n                \n\nb_Authoritarianism_Age\n                   0.07\n                  -0.05\n                   0.19\n                \n\nb_Authoritarianism_EduFactor1\n                   0.05\n                  -0.11\n                   0.22\n                \n\nb_Authoritarianism_EduFactor2\n                  -0.06\n                  -0.18\n                   0.07\n                \n\nb_Authoritarianism_Income\n                   0.04\n                  -0.07\n                   0.16\n                \n\nb_Dogmatism_Intercept\n                   0.04\n                  -0.12\n                   0.21\n                \n\nb_Dogmatism_SpeededIP\n                  -0.10\n                  -0.23\n                   0.03\n                \n\nb_Dogmatism_StrategicIP\n                  -0.09\n                  -0.22\n                   0.03\n                \n\nb_Dogmatism_Percep\n                   0.05\n                  -0.07\n                   0.16\n                \n\nb_Dogmatism_Caution\n                   0.06\n                  -0.05\n                   0.19\n                \n\nb_Dogmatism_Discount\n                   0.06\n                  -0.06\n                   0.17\n                \n\nb_Dogmatism_Gender1\n                  -0.01\n                  -0.13\n                   0.10\n                \n\nb_Dogmatism_Age\n                  -0.02\n                  -0.14\n                   0.11\n                \n\nb_Dogmatism_EduFactor1\n                   0.07\n                  -0.08\n                   0.24\n                \n\nb_Dogmatism_EduFactor2\n                  -0.03\n                  -0.16\n                   0.09\n                \n\nb_Dogmatism_Income\n                  -0.01\n                  -0.13\n                   0.10\n                \n\n\n\n\n    \n\n\nWe cannot directly check the percentage in the ROPE using the describe_posterior function for multivariate models yet - it will throw a warning. But we can use the rope function from the bayesTestR package.\n\nrope(mv1, \n     range = list(Conservatism = c(-0.05, 0.05), \n                  Authoritarianism = c(-0.05, 0.05), \n                  Dogmatism = c(-0.05, 0.05)), \n     ci = 1, ci_method = \"HDI\")\n## Possible multicollinearity between b_Authoritarianism_EduFactor1 and\n##   b_Authoritarianism_Intercept (r = 0.73), b_Dogmatism_EduFactor1 and\n##   b_Dogmatism_Intercept (r = 0.72). This might lead to inappropriate results. See\n##   'Details' in '?rope'.\n## # Proportion of samples inside the ROPE.\n## ROPE with depends on outcome variable.\n## \n## Parameter                    | inside ROPE |    ROPE width\n## ----------------------------------------------------------\n## Conservatism_Intercept       |     47.20 % | [-0.05, 0.05]\n## Conservatism_SpeededIP       |     44.52 % | [-0.05, 0.05]\n## Conservatism_StrategicIP     |      0.00 % | [-0.05, 0.05]\n## Conservatism_Percep          |     38.88 % | [-0.05, 0.05]\n## Conservatism_Caution         |      5.42 % | [-0.05, 0.05]\n## Conservatism_Discount        |     20.00 % | [-0.05, 0.05]\n## Conservatism_Gender1         |     23.50 % | [-0.05, 0.05]\n## Conservatism_Age             |      3.95 % | [-0.05, 0.05]\n## Conservatism_EduFactor1      |     46.92 % | [-0.05, 0.05]\n## Conservatism_EduFactor2      |     50.42 % | [-0.05, 0.05]\n## Conservatism_Income          |     53.60 % | [-0.05, 0.05]\n## Authoritarianism_Intercept   |     44.22 % | [-0.05, 0.05]\n## Authoritarianism_SpeededIP   |     20.35 % | [-0.05, 0.05]\n## Authoritarianism_StrategicIP |     16.25 % | [-0.05, 0.05]\n## Authoritarianism_Percep      |     58.45 % | [-0.05, 0.05]\n## Authoritarianism_Caution     |     34.85 % | [-0.05, 0.05]\n## Authoritarianism_Discount    |     10.22 % | [-0.05, 0.05]\n## Authoritarianism_Gender1     |     25.00 % | [-0.05, 0.05]\n## Authoritarianism_Age         |     33.92 % | [-0.05, 0.05]\n## Authoritarianism_EduFactor1  |     38.92 % | [-0.05, 0.05]\n## Authoritarianism_EduFactor2  |     41.58 % | [-0.05, 0.05]\n## Authoritarianism_Income      |     51.58 % | [-0.05, 0.05]\n## Dogmatism_Intercept          |     39.48 % | [-0.05, 0.05]\n## Dogmatism_SpeededIP          |     20.18 % | [-0.05, 0.05]\n## Dogmatism_StrategicIP        |     24.77 % | [-0.05, 0.05]\n## Dogmatism_Percep             |     46.95 % | [-0.05, 0.05]\n## Dogmatism_Caution            |     37.57 % | [-0.05, 0.05]\n## Dogmatism_Discount           |     40.30 % | [-0.05, 0.05]\n## Dogmatism_Gender1            |     60.52 % | [-0.05, 0.05]\n## Dogmatism_Age                |     54.55 % | [-0.05, 0.05]\n## Dogmatism_EduFactor1         |     33.88 % | [-0.05, 0.05]\n## Dogmatism_EduFactor2         |     49.88 % | [-0.05, 0.05]\n## Dogmatism_Income             |     58.98 % | [-0.05, 0.05]\n\nLet’s also create the same plot as before.\n\n\ndraws_mv1 &lt;- mv1 %&gt;%  \n  gather_draws(`b_.*`, regex = TRUE) %&gt;% \n  # for the plot I want to remove the intercepts\n  filter(!str_detect(.variable, \"Intercept\")) %&gt;%\n  # create a new variable that captures which outcome\n  mutate(outcome = case_when(\n    str_detect(.variable, \"Conservatism\") ~ \"Conservatism\",\n    str_detect(.variable, \"Authoritarianism\") ~ \"Authoritarianism\",\n    str_detect(.variable, \"Dogmatism\") ~ \"Dogmatism\")) %&gt;% \n  # create a new variable that captures which predictor\n  mutate(predictor = case_when(\n    str_detect(.variable, \"SpeededIP\") ~ \"Speed of Evidence Accumulation\",\n    str_detect(.variable, \"StrategicIP\") ~ \"Strategic Information Processing\",\n    str_detect(.variable, \"Percep\") ~ \"Perceptual Processing Time\",\n    str_detect(.variable, \"Caution\") ~ \"Caution\",\n    str_detect(.variable, \"Discount\") ~ \"Discount\")) %&gt;% \n  # create a new variable that captures which confounder\n  mutate(confounder = case_when(\n    str_detect(.variable, \"Age\") ~ \"Age\",\n    str_detect(.variable, \"Gender1\") ~ \"Gender\",\n    str_detect(.variable, \"EduFactor1\") ~ \"Education Level 1\",\n    str_detect(.variable, \"EduFactor2\") ~ \"Education Level 2\",\n    str_detect(.variable, \"Income\") ~ \"Income\")) %&gt;% \n  # set them as factors with specific levels\n  mutate(outcome = factor(outcome, levels = c(\n    \"Conservatism\",\n    \"Authoritarianism\",\n    \"Dogmatism\"))) %&gt;% \n  mutate(predictor = factor(predictor, levels = c(\n    \"Speed of Evidence Accumulation\",\n    \"Strategic Information Processing\",\n    \"Perceptual Processing Time\",\n    \"Caution\",\n    \"Discount\"))) %&gt;% \n  mutate(confounder = factor(confounder, levels = c(\n    \"Age\",\n    \"Gender\",\n    \"Education Level 1\",\n    \"Education Level 2\",\n    \"Income\"))) \n\nglimpse(draws_mv1)\n## Rows: 120,000\n## Columns: 8\n## Groups: .variable [30]\n## $ .chain     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ .iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20…\n## $ .draw      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20…\n## $ .variable  &lt;chr&gt; \"b_Conservatism_SpeededIP\", \"b_Conservatism_SpeededIP\", \"b_Conservati…\n## $ .value     &lt;dbl&gt; -0.02523, -0.02746, 0.11269, -0.01573, 0.07271, 0.04645, -0.00398, 0.…\n## $ outcome    &lt;fct&gt; Conservatism, Conservatism, Conservatism, Conservatism, Conservatism,…\n## $ predictor  &lt;fct&gt; Speed of Evidence Accumulation, Speed of Evidence Accumulation, Speed…\n## $ confounder &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\ndraws_mv1 %&gt;% \n  # drop NA (= predictors that I will not need) \n  drop_na(predictor) %&gt;%\n  ggplot(aes(x = .value, y = fct_rev(predictor), fill = predictor)) +\n  geom_rect(aes(xmin = -0.05, xmax = 0.05, ymin = -Inf, ymax = Inf), \n            color = \"transparent\", fill = bp[9], alpha = 0.05) +\n  geom_vline(xintercept = 0, alpha = 0.8) +\n  stat_halfeye(slab_alpha = 0.8,\n               .width = c(0.9, 0.95), point_interval = \"median_hdi\") +\n  scale_fill_manual(values = beyonce_palette(41, n = 7, type = \"continuous\")) +\n  guides(fill = \"none\", slab_alpha = \"none\", alpha = \"none\") +\n  labs(title = \"Associations of Cognitive Factors with Ideological Attitudes\", \n       x = \"Posterior Distribution\", \n       y = \"Predictor Variable\",\n       caption = \"90% and 95% credible intervals shown in black.\") +\n  scale_x_continuous(breaks = seq(-0.50, 0.50, by = 0.1)) +\n  coord_cartesian(xlim = c(-0.45, 0.45)) +\n  theme_ipsum(plot_title_size = 16,\n              axis_title_size = 14,\n              axis_title_face = \"bold\") +\n  annotate(\"text\", x = 0, y = 0.6, label = \"ROPE\", \n            color = \"black\", size = 4) +\n  facet_wrap(~ outcome, nrow=2)\n\n\n\n\n\n\n\nVery nice! From the plot we can directly infer that Strategic Information Processing is the most important predictor of Conservatism - this is also the overall most relevant effect! Our cognitive predictors do a worse job at predicting Authoritarianism or Dogmatism.\nWhat about our demographic predictor variables?\n\ndraws_mv1 %&gt;% \n  # drop NA (= predictors that I will not need) \n  drop_na(confounder) %&gt;%\n  ggplot(aes(x = .value, y = fct_rev(confounder), fill = confounder)) +\n  geom_rect(aes(xmin = -0.05, xmax = 0.05, ymin = -Inf, ymax = Inf), \n            color = \"transparent\", fill = bp[9], alpha = 0.05) +\n  geom_vline(xintercept = 0, alpha = 0.8) +\n  stat_halfeye(slab_alpha = 0.8,\n               .width = c(0.9, 0.95), point_interval = \"median_hdi\") +\n  scale_fill_manual(values = beyonce_palette(8, n = 5, type = \"continuous\")) +\n  guides(fill = \"none\", slab_alpha = \"none\", alpha = \"none\") +\n  labs(title = \"Associations of Demographic Factors with Ideological Attitudes\", \n       x = \"Posterior Distribution\", \n       y = \"Predictor Variable\",\n       caption = \"90% and 95% credible intervals shown in black.\") +\n  scale_x_continuous(breaks = seq(-0.50, 0.50, by = 0.1)) +\n  coord_cartesian(xlim = c(-0.45, 0.45)) +\n  theme_ipsum(plot_title_size = 16,\n              axis_title_size = 14,\n              axis_title_face = \"bold\") +\n  annotate(\"text\", x = 0, y = 0.6, label = \"ROPE\", \n            color = \"black\", size = 4) +\n  facet_wrap(~ outcome, nrow=2)\n\n\n\n\n\n\n\nVery neat! We see that there is a relatively high probability that Age is positively associated with Conservatism - this appears to be the most relevant effect again.\nChecking the model\nThe model did not throw any warnings, so I will let you check convergence etc. by yourself if you want to.\nBut let’s have a final look at the posterior predictive check:\n\npp_check(mv1, ndraws = 100, resp=\"Conservatism\")\n\n\n\n\n\n\n\n\npp_check(mv1, ndraws = 100, resp=\"Authoritarianism\")\n\n\n\n\n\n\n\nIt seems like the model does a bit worse at capturing Authoritarianism. This may be as Authoritarianism is actually a ordinal variable here that can only take the values 1, 2, 3, 4.\n\npp_check(mv1, ndraws = 100, resp=\"Dogmatism\")\n\n\n\n\n\n\n\nIndirect associations?\nOne thing that I did not talk about is that brms also allows to test indirect effects. Before conducting the study, we may have expected that Age is indirectly associated with Conservatism via cognitive abilities. In a causal analysis this would be called mediation, but for here I just want to say: Before you do something like this, try to make yourself aware of the conclusions that you can draw from such analyses in observational data. I may delve a bit deeper into this in a future blog post, but for now here are three reads I can highly recommend:\n\nThat’s a Lot to Process! Pitfalls of Popular Path Models by Rohrer et al. (2022)\nThinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data, again by Rohrer (2018)\nAnd as many people criticize statistical models that test for indirect associations in cross-sectional data, here a bit of a different view on it."
  },
  {
    "objectID": "blog/2024/03/30/index.html#sources-and-resources",
    "href": "blog/2024/03/30/index.html#sources-and-resources",
    "title": "Teaching Myself: Bayesian Multiple and Multivariate Linear Regression",
    "section": "Sources and Resources",
    "text": "Sources and Resources\nI am not a statistician, so please take all of what I wrote and coded here with a lot of caution. However, there are many great resources out there if you want to delve deeper into the topic - I am just getting started with this, but here are a few potentially helpful links.\n\nDoing Bayesian Data Analysis in brms and the tidyverse: A brms and tidyverse version by Solomon Kurz of the classic book by Kruschke Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. \nPractical Bayes Part I and Practical Bayes Part II: Great resources to dealing and avoiding common problems in Bayesian models by Micheal Clark\nBayesian Basics: General overview of how to run Bayesian models in R\nbayestestR: Vignettes on basic Bayesian models and their interpretation\nStan Wiki: Prior Choice: Go to reference for selecting priors\nCourse Handouts for Bayesian Data Analysis Class by Mark Lai\n\nAnd there are of course many many more resources, this is just to get started with some."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            April 16, 2024\n        \n        \n            Teaching Myself: Bayesian Path Modeling in blavaan and brms\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    Bayes\n                \n                \n                \n                    Path model\n                \n                \n                \n                    brms\n                \n                \n                \n                    blavaan\n                \n                \n            \n            \n\n            In previous blog posts, I looked into Bayesian linear regression - with a single predictor, multiple predictors, and a single and multiple outcome variables. blavaan and brms also allow to specify path models (i.e., structural equation models without latent variables) that allow to investigate more complex relationships between variables.\n            \n            \n        \n        \n    \n    \n    \n                  \n            March 30, 2024\n        \n        \n            Teaching Myself: Bayesian Multiple and Multivariate Linear Regression\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    Bayes\n                \n                \n                \n                    Regression\n                \n                \n                \n                    Multiple regression\n                \n                \n            \n            \n\n            In the last week, I ran a Bayesian Linear Regression with a single predictor and outcome variable. Usually, we investigate the effects of multiple predictors or control for confounding variables when the goal is to make an inference about and not to merely describe a relationship. In many cases, we also have more than one outcome variable. So let's take the next step and run a multiple linear and a multivariate regression in brms!\n            \n            \n        \n        \n    \n    \n    \n                  \n            March 19, 2024\n        \n        \n            Teaching Myself: Bayesian Linear Regression\n\n            \n            \n                \n                \n                    r\n                \n                \n                \n                    Bayes\n                \n                \n                \n                    Linear regression\n                \n                \n            \n            \n\n            As many psychologists, I was trained in frequentist regression. Here, I want to familiarise myself with Bayesian linear regression.\n            \n            \n        \n        \n    \n    \n    \n                  \n            March 7, 2024\n        \n        \n            Hello world\n\n            \n            \n                \n                \n                    News\n                \n                \n            \n            \n\n            I hope to share some insights here soon!\n            \n            \n        \n        \n    \n    \n\n\nNo matching items\n\n\n\nThe blog post listing is based on the website source of Andrew Heiss, and the tutorial and website of Marvin Schmitt, both of whom have created incredible resources under the CC-BY-SA 4.0 license. Thank you!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal website",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n\n\nOlaf Borghi, MSc\nI am an early stage researcher and PhD student in the MSCA doctoral network “Interdisciplinary Perspectives of the Politics of Adolescence & Democracy” at Royal Holloway, University of London. In this project I investigate the relative contributions of cognitive and emotional traits to ideological thinking across the developmental window of adolescence. I am particularly interested in how our current societal crises influence how young people form their political identities, which cognitive and emotional abilities underlie their political decision-making and belief-updating processes, and how we can better support the democratic involvement of young people.\nIf you are interested in my work or would like to collaborate, please feel free to contact me.\nResearch interests\n\nCognitive and emotional traits and their development\nMotivated reasoning and Bayesian updating\nIdeology, dogmatism and political decision making\n\nMethodological interests\n\nExperimental and longitudinal data analysis (using SEM and LMM)\nComputational modeling\nFunctional magnetic resonance imaging (fMRI)\n\n\nIP-PAD MSCA Doctoral Network Member"
  },
  {
    "objectID": "research/articles/borghi-etal-2024/index.html#important-links",
    "href": "research/articles/borghi-etal-2024/index.html#important-links",
    "title": "Day-to-Day Associations between Mindfulness and Perceived Stress: Insights from Random Intercept Cross-Lagged Panel Modeling",
    "section": "Important links",
    "text": "Important links\n\nPaper\nOSF repository"
  },
  {
    "objectID": "research/articles/borghi-etal-2024/index.html#abstract",
    "href": "research/articles/borghi-etal-2024/index.html#abstract",
    "title": "Day-to-Day Associations between Mindfulness and Perceived Stress: Insights from Random Intercept Cross-Lagged Panel Modeling",
    "section": "Abstract",
    "text": "Abstract\nObjective: Mindfulness is frequently seen as a protective factor of stress, but self-report measures of mindfulness may overlap with other related constructs, such as mental health, and could thus not only be a predictor, but also an outcome of stress. This study thus aimed to examine the longitudinal bidirectional associations between the use and perceived helpfulness of the four mindfulness facets Observe, Describe, Nonjudge, and Nonreact with daily perceived stress.\nMethods: Participants from a large (N = 1276) mixed student and community group sample filled out a brief daily diary over the time span of seven days. Bidirectional cross-lagged effects were investigated using the random-intercept cross-lagged panel model, an extension of the traditional cross-lagged panel model that allows to differentiate between stable between-unit differences and time-varying within-unit dynamics. In addition, we controlled for several baseline and sociodemographic confounders.\nResults: At the within-subject level, the use of Actaware was associated with higher perceived stress on the next day (β = .03, p = .029). The use (β = -.04, p = .025) and perceived helpfulness (β = -.05, p = .014) of Nonreact were associated with lower perceived stress on the next day. In turn, perceived stress was associated with lower perceived helpfulness of Describe (β = -.04, p = .037) and Nonreact (β = -.03, p = .038) on the next day. In addition, there were several residual correlations between mindfulness facets and perceived stress within days. At the between-subject level, there was a positive association between the random intercept of Describe and daily stress (r = .15, p = .003). In addition, while baseline perceived stress was negatively associated with the random intercepts of the mindfulness facets, two baseline components of mindfulness were not associated with the random intercept of perceived stress.\nConclusion: On the currently assessed timeframe, our results challenge prior results and assumptions regarding mindfulness as a buffering and protective factor against daily stress. With the exception of Nonreact, mindfulness was either positively associated with perceived stress, or in turn perceived stress appeared to interfere with the ability to stay mindful in daily life."
  },
  {
    "objectID": "research/articles/borghi-etal-2024/index.html#citation",
    "href": "research/articles/borghi-etal-2024/index.html#citation",
    "title": "Day-to-Day Associations between Mindfulness and Perceived Stress: Insights from Random Intercept Cross-Lagged Panel Modeling",
    "section": "Citation",
    "text": "Citation\n@article{borghietal_2024,\n  title = {Day-to-Day Associations between Mindfulness and Perceived Stress: Insights from Random Intercept Cross-Lagged Panel Modeling},\n  shorttitle = {Day-to-Day Associations between Mindfulness and Perceived Stress},\n  author = {Borghi, Olaf and Voracek, Martin and Tran, Ulrich S.},\n  year = {2024},\n  month = apr,\n  journal = {Frontiers in Psychology},\n  volume = {15},\n  publisher = {Frontiers},\n  issn = {1664-1078},\n  doi = {10.3389/fpsyg.2024.1272720},\n  urldate = {2024-04-16},\n  copyright = {All rights reserved},\n  langid = {english},\n  keywords = {Daily diary study,Daily hassles,Longitudinal,mindfulness,Random intercept cross-lagged panel model (RI-CLPM),stress,structural equation modeling (SEM)}\n}"
  },
  {
    "objectID": "research/posters/borghi-AON-master-thesis/index.html",
    "href": "research/posters/borghi-AON-master-thesis/index.html",
    "title": "Modulation of the Action Observation Network by Action, Agent and Observer Factors",
    "section": "",
    "text": "Poster of my master thesis project created for the Early Career Research Poster Session of the Faculty of Psychology, University of Vienna"
  }
]