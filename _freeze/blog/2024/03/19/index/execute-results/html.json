{
  "hash": "4deb7e10ea7e8eb94b79e9a62d019e0a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Teaching Myself: Bayesian Linear Regression\"\nauthor: \"Olaf Borghi\"\ndate: 2024-03-19\ndescription: \"As many psychologists, I was trained in frequentist regression. Here, I want to familiarise myself with Bayesian linear regression.\"\ncategories:\n  - r\n  - Bayes\n  - Linear regression\nfreeze: true\nformat:\n  html:\n    toc-depth: 4\nembed-resources: true\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# use groundhog to make code maximally reproducible\nif (!require(\"groundhog\", quietly = TRUE)) {\n  install.packages(\"groundhog\")\n}\nlibrary(\"groundhog\")\n\n# use groundhog to install and load packages\npkgs <- c(\"here\",         # Path management\n          \"tidyverse\",    # ggplot, dplyr, %>%, and friends\n          \"easystats\",    # bayestestR, performance, effectsize and friends\n          \"psych\",        # Descriptive statistics and other tools\n          \"brms\",         # Bayesian modeling through Stan\n          \"tidybayes\",    # Integration of Bayesian models in tidy + ggplot workflow\n          \"broom\",        # Convert model objects to data frames\n          \"broom.mixed\",  # Convert brms model objects to data frames\n          \"tinytable\",    # Lightweight package to create tables\n          \"hrbrthemes\",   # Additional ggplot themes\n          \"extrafont\",    # Additional fonts for plots etc\n          \"ggdist\",       # Special geoms for posterior distributions\n          \"ggrepel\",      # Automatically position labels\n          \"gghalves\",     # Special half geoms\n          \"patchwork\"     # Combine ggplot objects\n          )\n\ngroundhog.library(pkgs, \"2024-03-01\") # change to a recent date\n```\n:::\n\n\n\n\n\n\n## Teaching Myself: Bayesian Linear Regression\n\n### Meet the data\n\nI will use a large open data set described in [Eisenberg et al. (2018; 2019)](https://github.com/IanEisenberg/Self_Regulation_Ontology), combined with data from [Zmigrod et al. (2021)](https://zenodo.org/records/4434725/files/Data%20-%20Zmigrod%20et%20al%202021%20Philosophical%20Transactions.xlsx?download=1). The data from Eisenberg and colleagues includes measures from a large number of questionnaires and cognitive tasks that tap into the domain of self regulation. The data from Zmigrod and colleagues includes data from several measures of ideology.\n\n\n\n\n\nThe goal will be to find out whether self-report and task measures of self-control predict authoritarianism.\n\nVariables of interest:\n\n-   Self-reported self control: Brief Self-Control Scale\n\n-   Behavioral self-control: Accuracy in a Go / No-Go task, a cognitive task that requires a response to a frequent Go stimulus, such as pressing the space bar when a red square is displayed, and the inhibition of this response when a No-Go stimulus is displayed.\n\n-   Self reported authoritarianism.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata1 <- data1 %>%\n  rename(\"SubjectID\" = \"...1\")\n\ndata <- inner_join(data1, data2, by = \"SubjectID\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# select variables of interest\nd <- data %>%\n  select(c(SubjectID, BSCSSCON, GNGDRPIM, Authoritarianism)) %>%\n  rename(ID = SubjectID,\n         bscs = BSCSSCON,\n         gonogo = GNGDRPIM,\n         aut = Authoritarianism) %>%\n  mutate(bscs_z = scale(bscs),\n         gonogo_z = scale(gonogo),\n         aut_z = scale(aut))\n\ndescribe(d)\n##          vars   n   mean    sd median trimmed    mad   min    max  range  skew kurtosis\n## ID*         1 334 167.50 96.56 167.50  167.50 123.80  1.00 334.00 333.00  0.00    -1.21\n## bscs        2 334  46.07 10.30  47.00   46.33  11.86 18.00  65.00  47.00 -0.22    -0.53\n## gonogo      3 333   3.65  0.76   3.73    3.67   0.64  0.82   5.97   5.15 -0.11     1.18\n## aut         4 334   4.43  2.94   4.00    4.25   2.97  0.00  12.00  12.00  0.55    -0.05\n## bscs_z      5 334   0.00  1.00   0.09    0.02   1.15 -2.73   1.84   4.56 -0.22    -0.53\n## gonogo_z    6 333   0.00  1.00   0.10    0.03   0.85 -3.74   3.07   6.81 -0.11     1.18\n## aut_z       7 334   0.00  1.00  -0.15   -0.06   1.01 -1.51   2.58   4.08  0.55    -0.05\n##            se\n## ID*      5.28\n## bscs     0.56\n## gonogo   0.04\n## aut      0.16\n## bscs_z   0.05\n## gonogo_z 0.05\n## aut_z    0.05\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\nvars1_density<- d %>%\n  select(c(bscs_z, gonogo_z, aut_z)) %>%\n  pivot_longer(., cols = c(bscs_z, gonogo_z, aut_z), \n               names_to = \"Variable\", values_to = \"Value\") %>%\n  ggplot(aes(x = Value, fill = factor(Variable))) + \n  geom_density(aes(alpha = 0.6)) + \n  labs(x=\"Standardised values\", y=element_blank(),\n       title=\"Distributions of predictor and outcome variables\") +\n  scale_fill_ipsum() +\n  scale_color_ipsum() +\n  guides(color = \"none\", alpha = \"none\") +\n  theme_ipsum(plot_title_size = 14) + \n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\nvars1_density\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/variables1-densityplots-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nHave a first visual look of the association between the variables.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\nggplot(d, aes(x = bscs_z, y = aut_z)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\") +\n  labs(x=\"Brief Self-Control Scale\", y=\"Authoritarianism\",\n       title=\"Association between self-control and authoritarianism\") +\n  scale_fill_ipsum() +\n  scale_color_ipsum() +\n  guides(color = \"none\", alpha = \"none\") +\n  theme_ipsum(plot_title_size = 12)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/bscs-aut-scatterplot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\nggplot(d, aes(x = gonogo_z, y = aut_z)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\") +\n  labs(x=\"Go / No-Go Accuracy\", y=\"Authoritarianism\",\n       title=\"Association between Go / No-go accuracy and authoritarianism\") +\n  scale_fill_ipsum() +\n  scale_color_ipsum() +\n  guides(color = \"none\", alpha = \"none\") +\n  theme_ipsum(plot_title_size = 12)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/gng-aut-scatterplot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\nd %>%\n  select(c(bscs_z, gonogo_z, aut_z)) %>%\n  pairs.panels(.)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/psych-pairs-plot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n### A first Bayesian linear regression: One continuous predictor\n\nWe already displayed regression lines. So far, however, all of them were fit using the frequentist lm() function. We want to change that and go Bayesian here. Let's start by investigating the association between self-reported self-control and authoritarianism.\n\n#### Selecting priors\n\nI already scaled all variables of interest. This will make it a bit simpler to select priors. I will use weakly informative priors for our slope and intercept parameters (see <https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations>). The selected prior is $N(0,1)$.\n\nIt is also recommended to conduct a prior sensitivity analysis. This can be done for example by refitting the model with a different prior and by comparing the results. We will thus fit another model using $N(0,2)$ as my prior. Let's plot both of them to see what kind of an effect they may have on our parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# simulate some data\nx_values <- seq(-6, 6, length.out = 1000)\ndists <- data.frame(x = x_values,\n                    y1 = dnorm(x_values, mean = 0, sd = 1),\n                    y2 = dnorm(x_values, mean = 0, sd = 2))\n\n# colors\ncolors <- ipsum_pal()(2)\n\n# plot for N(0,1) \np1 <- ggplot(dists, aes(x = x, y = y1)) +\n  geom_area(aes(fill = \"N(0,1)\"), alpha = 0.8) +\n  scale_fill_manual(values = c(\"N(0,1)\" = colors[1])) +\n  labs(title = \"N(0,1)\", y = \"Density\", x = \"Value\") +\n  theme_ipsum() +\n  theme(legend.position = \"none\")\n\n# plot for N(0,2) \np2 <- ggplot(dists, aes(x = x, y = y2)) +\n  geom_area(aes(fill = \"N(0,2)\"), alpha = 0.8) +\n  scale_fill_manual(values = c(\"N(0,2)\" = colors[2])) +\n  labs(title = \"N(0,2)\", y = \"Density\", x = \"Value\") +\n  theme_ipsum() +\n  theme(legend.position = \"none\")\n\n# Combine plots \np_combined <- p1 + p2 + plot_layout(ncol = 2)\nprint(p_combined)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plt-priors-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe can see that in the the plot for $N(0,1)$, more density is around 0. As we expect our parameter values to be between 0 and 1, this gives more weight to parameter values closer to zero, and less weight to parameter values that are far from zero, compared to $N(0,2)$ , where more probability mass is also on parameters that are a bit further from 0. If we would know that the association is large, and at the same time our sample size is low, we could think of using informative priors.\n\nHere we prefer the prior with less information - this prior also has a slight regularizing effect.\n\nLet's check how our priors are set by default in `brms`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_prior(aut_z ~ bscs_z, data = d)\n##                    prior     class   coef group resp dpar nlpar lb ub       source\n##                   (flat)         b                                         default\n##                   (flat)         b bscs_z                             (vectorized)\n##  student_t(3, -0.1, 2.5) Intercept                                         default\n##     student_t(3, 0, 2.5)     sigma                               0         default\n```\n:::\n\n\nBy default `brms` sets non-informative flat priors for our slope parameters. We thus have to change this, and can do so directly when fitting the model.\n\n#### Fitting and summarising the first model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1.1 <- brm(aut_z ~ bscs_z, \n          data = d, \n          prior = c(prior(normal(0, 1), class = \"Intercept\"), \n                    prior(normal(0, 1), class = \"b\")),\n          seed = bayes_seed,\n          file = \"02_fits/fit_m1.1\")\n```\n:::\n\n\nCheck the results. We can use either `summary(m1.1)` or `print(m1.1)` to do so.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(m1.1, prob = 0.95)  # prob = 0.95 is the default\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: aut_z ~ bscs_z \n##    Data: d (Number of observations: 334) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.00      0.05    -0.11     0.11 1.00     4111     2956\n## bscs_z        0.15      0.05     0.05     0.26 1.00     3702     2562\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.99      0.04     0.92     1.07 1.00     3605     3062\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nUse the tools from `bayestestR` to have another look at the posterior. We also define a region of practical equivalence ranging from \\[-0.05; 0.05\\].\n\nMore on this later.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1.1_posteriors <- describe_posterior(m1.1, \n                                      centrality = \"median\",\n                                      dispersion = FALSE,\n                                      ci = 0.95,\n                                      ci_method = \"hdi\",\n                                      test = c(\"rope\"),\n                                      rope_range = c(-0.05, 0.05),\n                                      rope_ci = 1)\n\nm1.1_posteriors <- as.data.frame(m1.1_posteriors)\nm1.1_posteriors\n##     Parameter   Median   CI  CI_low CI_high ROPE_CI ROPE_low ROPE_high ROPE_Percentage\n## 2 b_Intercept 0.000595 0.95 -0.1085   0.103       1    -0.05      0.05          0.6392\n## 1    b_bscs_z 0.153896 0.95  0.0424   0.253       1    -0.05      0.05          0.0272\n##    Rhat  ESS\n## 2 1.000 4139\n## 1 0.999 3690\n```\n:::\n\n\nPlot the conditional effects.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nc_eff_m1.1 <- conditional_effects(m1.1)\nc_eff_m1.1_plot <- plot(c_eff_m1.1, \n                        points = T,\n                        point_args = list(size = 1, alpha = 1/4, width = .05, height = .05, color = \"black\"),\n                        plot = FALSE)[[1]] +\n  scale_color_ipsum() +\n  scale_fill_ipsum() +\n  theme_ipsum()\n  \nc_eff_m1.1_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/m1.1-c_eff-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nLet's use `bayesplot` functions to plot the posterior distributions of the parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\nmcmc_plot(m1.1, \n          type = \"areas\") +\n  theme_ipsum()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/m1.1_mcmc-plot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe can also manually create a similar plot using draws from the posterior distribution. For simplicity, Here we use `gather_draws` from the `tidybayes` package - this automatically returns a long dataframe suitable for plotting in ggplot.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndraws_m1.1 <- m1.1 %>% \n  gather_draws(`b_.*`, regex = TRUE)  %>% \n  mutate(variable = case_when(.variable == \"b_Intercept\" ~ \"Intercept\",\n                              .variable == \"b_bscs_z\"    ~ \"Self-Control\",\n                              TRUE  ~ .variable # Keeps the original value if it doesn't match any of the above\n                              ))\n\nggplot(draws_m1.1, aes(x = .value, y = fct_rev(variable), fill = variable)) +\n  geom_rect(aes(xmin = -0.05, xmax = 0.05, ymin = -Inf, ymax = Inf), fill = \"#FFE3EB\", alpha = 0.05) +\n  geom_vline(xintercept = 0) +\n  stat_halfeye(slab_alpha = 0.8,\n               .width = c(0.9, 0.95), point_interval = \"median_hdi\") +\n  scale_fill_ipsum() +\n  guides(fill = \"none\", slab_alpha = \"none\", alpha = \"none\") +\n  labs(title = \"Effect estimates\", x = \"Coefficient\", y = \"Variable\",\n       caption = \"90% and 95% credible intervals shown in black. Area in rose indicates ROPE.\") +\n  theme_ipsum(plot_title_size = 14,\n              axis_title_size = 12,\n              axis_title_face = \"bold\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/m1.1_mcmc-plot-manual-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n#### Checking the model\n\n##### Prior check\n\nLet's check if the prior that we used for the slope and intercept was informative or, as intended, only weakly informative. We can use functionality from `bayestestR` for this.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n\nprior_summary(m1.1)\n##                 prior     class   coef group resp dpar nlpar lb ub       source\n##          normal(0, 1)         b                                            user\n##          normal(0, 1)         b bscs_z                             (vectorized)\n##          normal(0, 1) Intercept                                            user\n##  student_t(3, 0, 2.5)     sigma                               0         default\ncheck_prior(m1.1)\n##     Parameter Prior_Quality\n## 1 b_Intercept uninformative\n## 2    b_bscs_z uninformative\n```\n:::\n\n\nOk - as intended our priors were uninformative.\n\n##### Posterior predictive check\n\nLet's also check how well our model performs in generating data that is similar to our observed data. We call this a **Posterior predictive check (PPC)**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npp_check(m1.1, ndraws= 200)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/m1.1-ppc1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npp_check(m1.1, ndraws = 100, type ='error_scatter_avg', alpha = .1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/m1.1-ppc2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n##### Trace plot\n\nCommonly, we look at the trace plots, i.e., the estimated values across each iteration for a chain. See [this nice practical guide by Michael Clark](https://m-clark.github.io/posts/2021-02-28-practical-bayes-part-i/#rhat) for some details. We will follow his code in several steps below.\n\nWhat should the trace plot look like? Ideally it should look like a series from a normal distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqplot(x = 1:1000, y = rnorm(1000), geom = 'line') +\n  theme_ipsum()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/trace-plot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNow we can also plot the trace plot from our model, using nice functionality from `bayesplot`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmcmc_plot(m1.1, type = 'trace') +\n  theme_ipsum()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/m1.1-traceplot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n#### Prior sensitivity checks\n\nOk, so we fit our first model. We also want to check whether the choice of different priors would have a substantial effect. We refit a similar model, just this time with $N(0,2)$ as prior for the intercept and slope parameter.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1.2 <- brm(aut_z ~ bscs_z, \n          data = d, \n          prior = c(prior(normal(0, 2), class = \"Intercept\"), \n                    prior(normal(0, 2), class = \"b\")),\n          seed = bayes_seed,\n          file = \"02_fits/fit_m1.2\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(m1.1, prob = 0.95)  # prob = 0.95 is the default\n##  Family: gaussian \n##   Links: mu = identity; sigma = identity \n## Formula: aut_z ~ bscs_z \n##    Data: d (Number of observations: 334) \n##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n##          total post-warmup draws = 4000\n## \n## Population-Level Effects: \n##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## Intercept     0.00      0.05    -0.11     0.11 1.00     4111     2956\n## bscs_z        0.15      0.05     0.05     0.26 1.00     3702     2562\n## \n## Family Specific Parameters: \n##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma     0.99      0.04     0.92     1.07 1.00     3605     3062\n## \n## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n## and Tail_ESS are effective sample size measures, and Rhat is the potential\n## scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nm1.2_posteriors <- describe_posterior(m1.2, \n                                      centrality = \"median\",\n                                      dispersion = FALSE,\n                                      ci = 0.95,\n                                      ci_method = \"hdi\",\n                                      test = c(\"rope\"),\n                                      rope_range = c(-0.05, 0.05),\n                                      rope_ci = 1)\n\nm1.2_posteriors <- as.data.frame(m1.2_posteriors)\n\n# Add a new column 'model' to each dataframe\nm1.1_posteriors <- m1.1_posteriors %>% mutate(model = \"m1.1\")\nm1.2_posteriors <- m1.2_posteriors %>% mutate(model = \"m1.2\")\n\n# Combine the dataframes\nm1.1_m1.2_combined <- bind_rows(m1.1_posteriors, m1.2_posteriors)\n\n# Print the combined dataframe\nm1.1_m1.2_combined %>% \n  select(Parameter, Median, CI_low, CI_high, model) %>% \n  tt(.)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!DOCTYPE html> \n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>tinytable_0omihsch5sn1oesd8yp3</title>\n    <style>\n.table td.tinytable_css_bnp89thhbzt00xqoxu9k, .table th.tinytable_css_bnp89thhbzt00xqoxu9k {    border-bottom: solid 0.1em #d3d8dc; }\n    </style>\n    <script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n    <script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n    <script>\n    MathJax = {\n      tex: {\n        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']]\n      },\n      svg: {\n        fontCache: 'global'\n      }\n    };\n    </script>\n  </head>\n\n  <body>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_0omihsch5sn1oesd8yp3\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\">Parameter</th>\n                <th scope=\"col\">Median</th>\n                <th scope=\"col\">CI_low</th>\n                <th scope=\"col\">CI_high</th>\n                <th scope=\"col\">model</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td>b_Intercept</td>\n                  <td>0.000595</td>\n                  <td>-0.1085</td>\n                  <td>0.103</td>\n                  <td>m1.1</td>\n                </tr>\n                <tr>\n                  <td>b_bscs_z</td>\n                  <td>0.153896</td>\n                  <td> 0.0424</td>\n                  <td>0.253</td>\n                  <td>m1.1</td>\n                </tr>\n                <tr>\n                  <td>b_Intercept</td>\n                  <td>0.000932</td>\n                  <td>-0.1045</td>\n                  <td>0.109</td>\n                  <td>m1.2</td>\n                </tr>\n                <tr>\n                  <td>b_bscs_z</td>\n                  <td>0.152366</td>\n                  <td> 0.0470</td>\n                  <td>0.265</td>\n                  <td>m1.2</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n\n    <script>\n      function styleCell_tinytable_n98ouu0pnt1f5nfqc339(i, j, css_id) {\n        var table = document.getElementById(\"tinytable_0omihsch5sn1oesd8yp3\");\n        table.rows[i].cells[j].classList.add(css_id);\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_0omihsch5sn1oesd8yp3');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_tinytable_n98ouu0pnt1f5nfqc339(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_0omihsch5sn1oesd8yp3\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n\nwindow.addEventListener('load', function () { styleCell_tinytable_n98ouu0pnt1f5nfqc339(0, 0, 'tinytable_css_bnp89thhbzt00xqoxu9k') })\nwindow.addEventListener('load', function () { styleCell_tinytable_n98ouu0pnt1f5nfqc339(0, 1, 'tinytable_css_bnp89thhbzt00xqoxu9k') })\nwindow.addEventListener('load', function () { styleCell_tinytable_n98ouu0pnt1f5nfqc339(0, 2, 'tinytable_css_bnp89thhbzt00xqoxu9k') })\nwindow.addEventListener('load', function () { styleCell_tinytable_n98ouu0pnt1f5nfqc339(0, 3, 'tinytable_css_bnp89thhbzt00xqoxu9k') })\nwindow.addEventListener('load', function () { styleCell_tinytable_n98ouu0pnt1f5nfqc339(0, 4, 'tinytable_css_bnp89thhbzt00xqoxu9k') })\n    </script>\n\n  </body>\n\n</html>\n```\n\n:::\n:::\n\n\nWe can see that irrespective of our prior choice, both the median of the posterior distributions of the two different models, and the 95% HDI are very similar.\n\n## Takeaway\n\n-   Using the Bayesian approach we get much more than just a point estimate for our slope parameter of interest - we get a posterior distribution of parameters that are plausible given our observed data.\n\n-   An increase of self control of 1 SD is associated with an increase of 0.04-0.26 SDs (median: 0.15) in authoritarianism.\n\n-   The 95% HDI of our predictor of interest \"Self-Control\" does not include 0. We can thus say with a relatively high probability (according to common criteria - some authors also argue for 89% or 90% CIs) given our data that the effect is different from 0.\n\n-   This is still true when we define and look at the ROPE - a region of practical equivalence. There is only minor overlap between the posterior distribution of our parameter and the ROPE (around 3%), indicating that the probability that our effect is practically relevant given our defined criteria is large.\n\n## Resources\n\nI am not a statistician, so please take all of what I wrote and coded here with a lot of caution. However, there are many great resources out there if you want to delve deeper into the topic - I am just getting started with this, but here are a few potentially helpful links.\n\n-   [*Doing Bayesian Data Analysis* in brms and the tidyverse](https://bookdown.org/content/3686/): A brms and tidyverse version by Solomon Kurz of the classic book by Kruschke [*Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan*](https://sites.google.com/site/doingbayesiandataanalysis/). \n\n-   [Practical Bayes Part I](https://m-clark.github.io/posts/2021-02-28-practical-bayes-part-i/#overview) and [Practical Bayes Part II](https://m-clark.github.io/posts/2021-02-28-practical-bayes-part-ii/): Great resources to dealing and avoiding common problems in Bayesian models by Micheal Clark\n\n-   [Bayesian Basics](https://m-clark.github.io/bayesian-basics/preface.html): General overview of how to run Bayesian models in R\n\n-   [bayestestR](https://easystats.github.io/bayestestR/index.html): Vignettes on basic Bayesian models and their interpretation\n\n-   [Stan Wiki: Prior Choice](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations): Go to reference for selecting priors\n\n-   [Course Handouts for Bayesian Data Analysis Class](https://bookdown.org/marklhc/notes_bookdown/) by Mark Lai\n\nAnd there are of course many many more resources, this is just to get started with some.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}